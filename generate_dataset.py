import os
import json
import random
import pandas as pd
from pymongo import MongoClient
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# Load environment variables
load_dotenv()

MONGO_URL = os.getenv("MONGO_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "astrobot_original"  # Correct DB for content chunks
# Based on multimodel_rag.py
COLLECTIONS = ["original_text_chunks", "original_image_chunks", "original_table_chunks"]

KEYWORDS = ["‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏µ‡πÄ‡∏Å‡∏¥‡∏î", "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î", "‡∏Å‡∏≤‡∏£‡∏á‡∏≤‡∏ô", "‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å", "‡∏™‡∏µ‡∏°‡∏á‡∏Ñ‡∏•"]
TARGET_COUNT = 10

def get_mongo_client():
    try:
        client = MongoClient(MONGO_URL)
        # Verify connection
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {MONGO_URL.split('@')[-1]}")  # Hide credentials
        return client
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return None

def fetch_candidate_chunks(client, db_name, collections, keywords, limit_per_keyword=50):
    db = client[db_name]
    candidates = []
    
    print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB Database: '{db_name}'...")
    
    for collection_name in collections:
        if collection_name not in db.list_collection_names():
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Collection: {collection_name} - ‡∏Ç‡πâ‡∏≤‡∏°")
            continue
            
        collection = db[collection_name]
        doc_count = collection.count_documents({})
        print(f"   üìÇ Collection '{collection_name}' ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {doc_count} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        
        for keyword in keywords:
            # Simple text search regex
            query = {"text": {"$regex": keyword, "$options": "i"}}
            cursor = collection.find(query).limit(limit_per_keyword)
            found_docs = list(cursor)
            
            if found_docs:
                print(f"      - Keyword '{keyword}': ‡∏û‡∏ö {len(found_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
                for doc in found_docs:
                    # Avoid duplicates if traversing multiple keywords
                    if not any(c['_id'] == doc['_id'] for c in candidates):
                        candidates.append({
                            "_id": str(doc['_id']),
                            "text": doc.get('text', ''),
                            "source": doc.get('source', 'Unknown'),
                            "page": doc.get('page', 'N/A'),
                            "collection": collection_name,
                            "matched_keyword": keyword
                        })
    
    print(f"‚úÖ ‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(candidates)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    
    # Verification: Print the first retrieved document to prove it comes from MongoDB
    if candidates:
        sample = candidates[0]
        print("\nüîé [Verification] ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB:")
        print(f"   üÜî ID: {sample['_id']}")
        print(f"   üìÇ Collection: {sample['collection']}")
        print(f"   üìÑ Source: {sample['source']}")
        print(f"   üìù Text (Snippet): {sample['text'][:200]}...")
        print("--------------------------------------------------\n")
        
    return candidates

def generate_qa_pair(client_openai, context):
    prompt = f"""
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
    ---------------------
    {context}
    ---------------------
    ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ (‡πÅ‡∏•‡∏∞‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)
    ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°" ‡πÅ‡∏•‡∏∞ "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 1 ‡∏Ñ‡∏π‡πà
    
    ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Critical Requirement):
    1. **‡∏ï‡πâ‡∏≠‡∏á** ‡∏™‡∏°‡∏°‡∏ï‡∏¥ "‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏µ‡πÄ‡∏Å‡∏¥‡∏î" ‡πÉ‡∏™‡πà‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö **DD/MM/YYYY** (‡πÄ‡∏ä‡πà‡∏ô "‡∏Ñ‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15/04/1990...") ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
    2. ‡∏´‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á "‡∏£‡∏≤‡∏®‡∏µ" ‡πÉ‡∏î ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏≤‡∏®‡∏µ‡∏ô‡∏±‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏§‡∏©‡∏† -> 15/05/xxxx) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
    3. **‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Answer)** ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** (‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏≤‡∏ï‡∏≠‡∏ö ‡πÅ‡∏°‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Å‡πá‡∏ï‡∏≤‡∏°) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• Faithfulness ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°
    4. ‡∏´‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏î‡∏≤‡∏ß) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î ‡πÉ‡∏™‡πà‡∏•‡∏á‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏â‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    
    ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏†‡∏≤‡∏©‡∏≤:
    - ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î
    
    Format the output as JSON:
    {{
        "question": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î",
        "answer": "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
    }}
    """
    
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates Q&A pairs from text."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"Error generating Q&A: {e}")
        return None

def main():
    if not MONGO_URL:
        print("‚ùå Error: MONGO_URL not found in environment variables.")
        return
    if not OPENAI_API_KEY:
        print("‚ùå Error: OPENAI_API_KEY not found in environment variables.")
        return

    mongo_client = get_mongo_client()
    if not mongo_client:
        return

    # 1. Fetch Data from MongoDB
    print("\n--- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---")
    candidates = fetch_candidate_chunks(mongo_client, DB_NAME, COLLECTIONS, KEYWORDS)
    
    if len(candidates) < TARGET_COUNT:
        print(f"‚ö†Ô∏è ‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏µ‡∏¢‡∏á {len(candidates)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏ã‡∏∂‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ {TARGET_COUNT} ‡∏Ç‡πâ‡∏≠")
        print("   ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏ã‡πâ‡∏≥‡∏´‡∏≤‡∏Å‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô)")
    
    # Select samples (ensure diversity if possible, or just take random allowed)
    # If we have enough, sample w/o replacement. If not, we might need to reuse or just clamp.
    if len(candidates) >= TARGET_COUNT:
        selected_chunks = random.sample(candidates, TARGET_COUNT)
    else:
        selected_chunks = candidates # Take all
    
    print(f"\n--- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Q&A ‡∏î‡πâ‡∏ß‡∏¢ LLM ({len(selected_chunks)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£) ---")
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    
    dataset = []
    
    for i, chunk in enumerate(tqdm(selected_chunks, desc="Generating")):
        try:
            qa = generate_qa_pair(openai_client, chunk['text'])
            if qa:
                dataset.append({
                    "question": qa['question'],
                    "ground_truth": qa['answer'], # Ragas expects 'ground_truth' or 'ground_truths' usually, but for simple viewing 'answer' is fine. We will use 'answer' for CSV
                    "answer": qa['answer'],      # Keeping 'answer' for clear CSV reading
                    "context": chunk['text'],    # Ragas expects 'contexts' as list of strings
                    "source_page": chunk['page'],
                    "collection": chunk['collection'],
                    "keyword": chunk['matched_keyword']
                })
        except Exception as e:
            print(f"Skipping chunk {i}: {e}")

    # Output to files
    df = pd.DataFrame(dataset)
    
    # Create final JSON structure for Ragas (if we were using the HF dataset loader directly, but simple JSON/CSV is fine for our custom evaluation script)
    # We will save as simple records
    
    print("\n--- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---")
    print(f"üìä ‡πÑ‡∏î‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(df)} ‡∏Ç‡πâ‡∏≠")
    
    csv_path = "generated_dataset.csv"
    json_path = "generated_dataset.json"
    
    df.to_csv(csv_path, index=False, encoding='utf-8-sig') # utf-8-sig for Excel Thai support
    df.to_csv(csv_path, index=False, encoding='utf-8-sig') # utf-8-sig for Excel Thai support
    
    # Use json.dump for cleaner formatting (no escaped /)
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà: {os.path.abspath(csv_path)}")
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà: {os.path.abspath(json_path)}")

if __name__ == "__main__":
    main()
