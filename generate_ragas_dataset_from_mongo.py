"""
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á RAGAS Dataset ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô MongoDB

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ workflow ‡πÉ‡∏´‡∏°‡πà:
1. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ (DD/MM/YYYY ‡πÅ‡∏•‡∏∞ HH:MM)
2. ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB astrobot_summary
3. ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAGAS evaluation

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: 
- ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô collection (‡∏î‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings)
- ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô ask_question_to_rag)
- ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ (ask_question_to_rag) ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏≠‡∏á
- ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
- **‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ
"""

import os
import json
import sys
import random
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from datetime import datetime, date
from pymongo import MongoClient
import logging
import numpy as np

# ‡πÇ‡∏´‡∏•‡∏î environment variables
load_dotenv()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô DD/MM/YYYY (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ú‡πà‡∏≤‡∏ô env)
def generate_random_birth_date(
    start_year: int = 1950,
    end_year: int = 2010,
) -> str:
    """‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY"""
    year = random.randint(start_year, end_year)
    month = random.randint(1, 12)
    # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏µ‡∏≠‡∏ò‡∏¥‡∏Å‡∏™‡∏∏‡∏£‡∏ó‡∏¥‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå)
    days_in_month = [
        31,
        29 if (year % 400 == 0 or (year % 4 == 0 and year % 100 != 0)) else 28,
        31, 30, 31, 30, 31, 31, 30, 31, 30, 31,
    ]
    day = random.randint(1, days_in_month[month - 1])
    return f"{day:02d}/{month:02d}/{year:04d}"

# Import config
from config import SUMMARY_DB_NAME

# Import OpenAI
try:
    from openai import OpenAI
except ImportError:
    logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö OpenAI library. ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install openai")
    sys.exit(1)


def get_mongodb_data() -> List[Dict[str, Any]]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB collections ‡πÉ‡∏ô astrobot_summary ‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô)
    
    Returns:
        List[Dict]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB ‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡πâ‡∏ß
    """
    mongo_uri = os.getenv("MONGO_URL")
    if not mongo_uri or mongo_uri == "mongodb+srv://your-username:your-password@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority":
        logger.error("‚ùå MONGO_URL ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default")
        return []
    
    try:
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=10000)
        db = client[SUMMARY_DB_NAME]
        
        collections_to_search = [
            "processed_text_chunks",
            "processed_image_chunks",
            "processed_table_chunks",
        ]
        
        all_data = []
        
        for collection_name in collections_to_search:
            try:
                collection = db[collection_name]
                
                # ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings field ‡πÅ‡∏•‡∏∞ embeddings ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô None
                query = {
                    "embeddings": {"$exists": True, "$ne": None}
                }
                
                doc_count_with_embeddings = collection.count_documents(query)
                
                if doc_count_with_embeddings == 0:
                    logger.warning(f"‚ö†Ô∏è Collection {collection_name} ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings")
                    continue
                
                # ‚úÖ ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô)
                docs = list(collection.find(query))
                
                docs_with_content = 0
                for doc in docs:
                    # ‚úÖ ‡πÉ‡∏ä‡πâ summary ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ä‡πâ text
                    text_content = doc.get("summary") or doc.get("text", "")
                    
                    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô list/array ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á
                    embeddings = doc.get("embeddings")
                    if not embeddings or (isinstance(embeddings, list) and len(embeddings) == 0):
                        logger.warning(f"‚ö†Ô∏è ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ chunk_id={doc.get('chunk_id', 'unknown')} ‡∏°‡∏µ embeddings field ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á - ‡∏Ç‡πâ‡∏≤‡∏°")
                        continue
                    
                    if text_content and text_content.strip():
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡∏à‡∏£‡∏¥‡∏á‡πÜ
                        doc_source = doc.get("source", "unknown")
                        doc_page = doc.get("page", None)
                        doc_chunk_id = doc.get("chunk_id", 0)
                        
                        all_data.append({
                            "content": text_content.strip(),
                            "type": collection_name.replace("processed_", "").replace("_chunks", ""),
                            "source": doc_source,
                            "chunk_id": doc_chunk_id,
                            "page": doc_page,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° page number
                            "embeddings": embeddings,  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity
                            "_mongodb_id": str(doc.get("_id", "")),  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö MongoDB _id ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                        })
                        docs_with_content += 1
                        
                        # Log ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• source ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debugging
                        if docs_with_content <= 3:  # ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏£‡∏Å
                            logger.debug(f"   üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: page={doc_page}, source={doc_source}, chunk_id={doc_chunk_id}, content_length={len(text_content)}")
                
                logger.info(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {collection_name}: {docs_with_content}/{len(docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error reading from {collection_name}: {e}")
                continue
        
        client.close()
        
        logger.info(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_data)} chunks (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡πâ‡∏ß)")
        return all_data
        
    except Exception as e:
        logger.error(f"‚ùå Error connecting to MongoDB: {e}")
        import traceback
        traceback.print_exc()
        return []


def generate_qa_pairs_with_llm(
    content: str,
    content_type: str,
    openai_client: OpenAI,
    num_questions: int = 2,
    invalid_answer_phrases: List[str] = None
) -> List[Dict[str, str]]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    
    Args:
        content: ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        content_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (text/image/table)
        openai_client: OpenAI client
        num_questions: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        
    Returns:
        List[Dict]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    """
    try:
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ prompt ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        max_content_length = 2000
        truncated_content = content[:max_content_length]
        if len(content) > max_content_length:
            truncated_content += "..."
        
        # ‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
        # ‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥
        birth_date = generate_random_birth_date()
        
        # ‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        additional_dates = [generate_random_birth_date() for _ in range(3)]
        
        # ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î (HH:MM)
        def generate_random_birth_time():
            hour = random.randint(0, 23)
            minute = random.choice([0, 15, 30, 45])
            return f"{hour:02d}:{minute:02d}"
        
        birth_times = [generate_random_birth_time() for _ in range(3)]
        
        birth_info = f"""‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á):
- ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å: {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[0]} ‡∏ô.
- ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {additional_dates[0]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[1]} ‡∏ô., {additional_dates[1]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[2]} ‡∏ô., {additional_dates[2]}

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô**
"""

        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå

**‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB database (astrobot_summary) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**

{birth_info}‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏°‡∏≤) ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á

**üìã ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
1. **‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
2. **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB** - ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB collection (processed_text_chunks, processed_image_chunks, ‡∏´‡∏£‡∏∑‡∏≠ processed_table_chunks)
3. **‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤** - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
4. **‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•** - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB) ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà**‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏µ‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î**‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
- **‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ** - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î, ‡∏£‡∏≤‡∏®‡∏µ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î, ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ
- **‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤** - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ (Ascendant) ‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î, ‡∏£‡∏≤‡∏®‡∏µ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏•‡∏±‡∏Ñ‡∏ô‡∏≤
- **‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î** - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤, ‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 12 ‡∏ö‡πâ‡∏≤‡∏ô
- **‡∏£‡∏≤‡∏®‡∏µ‡∏ó‡∏±‡πâ‡∏á 12 ‡∏£‡∏≤‡∏®‡∏µ** (‡πÄ‡∏°‡∏©, ‡∏û‡∏§‡∏©‡∏†, ‡πÄ‡∏°‡∏ñ‡∏∏‡∏ô, ‡∏Å‡∏£‡∏Å‡∏é, ‡∏™‡∏¥‡∏á‡∏´‡πå, ‡∏Å‡∏±‡∏ô‡∏¢‡πå, ‡∏ï‡∏∏‡∏•, ‡∏û‡∏¥‡∏à‡∏¥‡∏Å, ‡∏ò‡∏ô‡∏π, ‡∏°‡∏±‡∏á‡∏Å‡∏£, ‡∏Å‡∏∏‡∏°‡∏†‡πå, ‡∏°‡∏µ‡∏ô) - ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î
- **‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ (Ascendant) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏•‡∏±‡∏Ñ‡∏ô‡∏≤** - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏•‡∏±‡∏Ñ‡∏ô‡∏≤
- **‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 12 ‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤** - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î
- **‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢** - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
- **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ** - ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î
- **‡∏ò‡∏≤‡∏ï‡∏∏‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏®‡∏µ** - ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î
- **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î** - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì, ‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°, ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î

**‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB:**
{truncated_content}

**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:** {content_type}

**‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
1. **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ** - ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
2. **‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô** - ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
3. **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î):**
   - "‡∏£‡∏≤‡∏®‡∏µ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[0]} ‡∏ô. ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î {additional_dates[0]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[1]} ‡∏ô. ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏®‡∏µ‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î {additional_dates[1]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[2]} ‡∏ô. ‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
   - "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[0]} ‡∏ô. ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {additional_dates[0]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[1]} ‡∏ô. ‡∏°‡∏µ‡∏£‡∏≤‡∏®‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
   - "‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 12 ‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[0]} ‡∏ô. ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î {additional_dates[1]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[2]} ‡∏ô. ‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
   - "‡∏ò‡∏≤‡∏ï‡∏∏‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏®‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[0]} ‡∏ô. ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏•‡∏±‡∏Ñ‡∏ô‡∏≤‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î {additional_dates[2]} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[1]} ‡∏ô. ‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
   - "‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {birth_date} ‡πÄ‡∏ß‡∏•‡∏≤ {birth_times[2]} ‡∏ô. ‡∏°‡∏µ‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?"
   - "‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
4. **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢** - ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô, ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 08:00, 14:30, 22:15), ‡πÅ‡∏•‡∏∞‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
5. **‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î** - ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 70% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î
6. **‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
7. **‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà" ‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô** - ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
8. **‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á** ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô:
   - "‡∏£‡∏≤‡∏®‡∏µ‡∏Å‡∏£‡∏Å‡∏é‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏≤‡∏° "‡∏£‡∏≤‡∏®‡∏µ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà XX/XX/XXXX ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?" ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ô‡∏±‡πâ‡∏ô)
   - "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏≤‡∏° "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î XX/XX/XXXX ‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á)
   - "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏≤‡∏° "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà XX/XX/XXXX ‡πÄ‡∏ß‡∏•‡∏≤ XX:XX ‡∏ô. ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?" ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á)
9. **contexts ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ** - ‡∏≠‡∏¢‡πà‡∏≤‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
10. ‡∏™‡∏£‡πâ‡∏≤‡∏á {num_questions} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
11. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
12. **‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó** - text, image (OCR), table - ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å MongoDB

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (JSON):**
[
  {{
    "question": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB",
    "answer": "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å)",
    "contexts": ["‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"]
  }}
]

**‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö contexts:**
- contexts ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ
- contexts ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
- contexts ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏®‡∏µ, ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤)
- **‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà" ‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô** - ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- **‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á** ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ (‡πÄ‡∏ä‡πà‡∏ô "‡∏£‡∏≤‡∏®‡∏µ‡∏Å‡∏£‡∏Å‡∏é‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞)

**‚ö†Ô∏è ‡∏à‡∏≥‡πÑ‡∏ß‡πâ: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ (‡∏à‡∏≤‡∏Å MongoDB) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ**

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô:"""

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ OpenAI
        openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        
        response = openai_client.chat.completions.create(
            model=openai_model,
            messages=[
                {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ (‡∏à‡∏≤‡∏Å MongoDB) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500,
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # ‡∏•‡∏ö markdown code blocks ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        # Parse JSON
        try:
            qa_pairs = json.loads(response_text)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
            if not isinstance(qa_pairs, list):
                qa_pairs = [qa_pairs]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
            valid_pairs = []
            for pair in qa_pairs:
                if isinstance(pair, dict) and "question" in pair and "answer" in pair:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"
                    answer = str(pair.get("answer", "")).strip()
                    # ‡πÉ‡∏ä‡πâ invalid_answer_phrases ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ default
                    check_phrases = invalid_answer_phrases if invalid_answer_phrases else [
                        "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                        "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤",
                        "‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ",
                        "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤",
                        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà", "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ",
                        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏®‡∏µ",
                        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ"
                    ]
                    if any(phrase in answer for phrase in check_phrases):
                        logger.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {pair.get('question', '')[:50]}...")
                        logger.warning(f"   ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:100]}...")
                        continue
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° contexts
                    if "contexts" not in pair or not pair["contexts"]:
                        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô context ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
                        pair["contexts"] = [truncated_content[:500]]
                    else:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ contexts ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
                        pair["contexts"] = [ctx for ctx in pair["contexts"] if ctx and ctx.strip()]
                        if not pair["contexts"]:
                            # ‡∏ñ‡πâ‡∏≤ contexts ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                            pair["contexts"] = [truncated_content[:500]]
                    
                    valid_pairs.append(pair)
            
            return valid_pairs
            
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è Error parsing JSON response: {e}")
            logger.warning(f"Response text: {response_text[:200]}...")
            return []
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error generating QA pairs: {e}")
        import traceback
        traceback.print_exc()
        return []


def retrieve_from_mongodb_with_embedding(
    question: str,
    embedding_model,
    top_k: int = 5,
    threshold: float = 0.10
) -> List[Dict[str, Any]]:
    """
    Retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ embedding model (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô ask_question_to_rag)
    
    Args:
        question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        embedding_model: SentenceTransformer model
        top_k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á
        threshold: threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö similarity score
        
    Returns:
        List[Dict]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏° similarity score
    """
    try:
        import numpy as np
        from pymongo import MongoClient
        
        mongo_uri = os.getenv("MONGO_URL")
        if not mongo_uri or mongo_uri == "mongodb+srv://your-username:your-password@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority":
            logger.error("‚ùå MONGO_URL ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default")
            return []
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á query embedding
        query_embedding = embedding_model.encode(question, convert_to_numpy=True)
        logger.debug(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á query embedding ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏Ç‡∏ô‡∏≤‡∏î: {len(query_embedding)} dimensions)")
        
        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=10000)
        db = client[SUMMARY_DB_NAME]
        
        collections_to_search = [
            "processed_text_chunks",
            "processed_image_chunks",
            "processed_table_chunks",
        ]
        
        all_retrieved_docs = []
        
        for collection_name in collections_to_search:
            try:
                collection = db[collection_name]
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ embeddings
                query = {
                    "embeddings": {"$exists": True, "$ne": None}
                }
                docs = list(collection.find(query))
                
                if not docs:
                    continue
                
                logger.debug(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô collection: {collection_name} ({len(docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)")
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity scores
                similarities = []
                for doc in docs:
                    if 'embeddings' not in doc:
                        continue
                    
                    try:
                        doc_embedding = np.array(doc['embeddings'])
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ dimensions ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
                        if len(doc_embedding) != len(query_embedding):
                            continue
                        
                        # Cosine similarity
                        similarity = np.dot(query_embedding, doc_embedding) / (
                            np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                        )
                        similarities.append((similarity, doc))
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Error calculating similarity: {e}")
                        continue
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° similarity score
                similarities.sort(key=lambda x: x[0], reverse=True)
                
                # ‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î top_k ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
                top_docs = similarities[:top_k]
                
                for similarity, doc in top_docs:
                    if similarity < threshold:
                        continue
                    
                    # ‡πÉ‡∏ä‡πâ summary ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ä‡πâ text
                    text_content = doc.get("summary") or doc.get("text", "")
                    
                    if not text_content or not text_content.strip():
                        continue
                    
                    source_info = doc.get("source", "unknown")
                    page_number = doc.get("page", None)
                    chunk_id = doc.get("chunk_id", 0)
                    
                    all_retrieved_docs.append({
                        "content": text_content.strip(),
                        "similarity": float(similarity),
                        "source": source_info,
                        "page": page_number,
                        "chunk_id": chunk_id,
                        "collection": collection_name,
                        "_mongodb_id": str(doc.get("_id", "")),
                    })
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error reading from {collection_name}: {e}")
                continue
        
        client.close()
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° similarity ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å collection)
        all_retrieved_docs.sort(key=lambda x: x["similarity"], reverse=True)
        
        # ‡πÄ‡∏≠‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î top_k
        all_retrieved_docs = all_retrieved_docs[:top_k]
        
        logger.debug(f"‚úÖ Retrieve ‡πÑ‡∏î‡πâ {len(all_retrieved_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        return all_retrieved_docs
        
    except Exception as e:
        logger.error(f"‚ùå Error retrieving from MongoDB: {e}")
        import traceback
        traceback.print_exc()
        return []


def generate_questions_with_llm(
    openai_client: OpenAI,
    num_questions: int = 10,
    topics: List[str] = None
) -> List[str]:
    """
    ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ
    
    **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY ‡πÅ‡∏•‡∏∞ HH:MM
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ
    
    Args:
        openai_client: OpenAI client
        num_questions: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á
        topics: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏ñ‡πâ‡∏≤ None ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
        
    Returns:
        List[str]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)
    """
    try:
        if topics is None:
            topics = [
                "‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ",
                "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤",
                "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤",
                "‡∏£‡∏≤‡∏®‡∏µ‡∏ó‡∏±‡πâ‡∏á 12 ‡∏£‡∏≤‡∏®‡∏µ",
                "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ (Ascendant)",
                "‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 12 ‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤",
                "‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢",
                "‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏®‡∏µ",
                "‡∏ò‡∏≤‡∏ï‡∏∏‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏®‡∏µ",
            ]
        
        topics_str = "\n".join([f"- {topic}" for topic in topics])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
        import random
        def generate_random_birth_date():
            year = random.randint(1990, 2010)
            month = random.randint(1, 12)
            days_in_month = [31, 29 if (year % 400 == 0 or (year % 4 == 0 and year % 100 != 0)) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
            day = random.randint(1, days_in_month[month - 1])
            return f"{day:02d}/{month:02d}/{year:04d}"
        
        def generate_random_birth_time():
            hour = random.randint(0, 23)
            minute = random.choice([0, 15, 30, 45])
            return f"{hour:02d}:{minute:02d}"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ä‡∏∏‡∏î
        example_dates = [generate_random_birth_date() for _ in range(5)]
        example_times = [generate_random_birth_time() for _ in range(5)]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        example_questions = []
        for i in range(min(3, len(example_dates))):
            example_questions.append(f"- \"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡∏ß‡∏á {example_dates[i]} ‡πÄ‡∏ß‡∏•‡∏≤ {example_times[i]}\"")
            example_questions.append(f"- \"‡∏£‡∏≤‡∏®‡∏µ‡∏≠‡∏∞‡πÑ‡∏£ {example_dates[i]} ‡πÄ‡∏ß‡∏•‡∏≤ {example_times[i]}\"")
            example_questions.append(f"- \"‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ {example_dates[i]} ‡πÄ‡∏ß‡∏•‡∏≤ {example_times[i]}\"")
        
        examples_str = "\n".join(example_questions)
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
{topics_str}

**‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
1. **‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ** - ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY ‡πÅ‡∏•‡∏∞ HH:MM
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
3. ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î ‡∏£‡∏≤‡∏®‡∏µ ‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ ‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤
4. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
6. **‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢** - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÜ

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î:**
- ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î: DD/MM/YYYY (‡πÄ‡∏ä‡πà‡∏ô 07/09/2003, 15/08/1995, 20/12/2000)
- ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î: HH:MM (‡πÄ‡∏ä‡πà‡∏ô 14:30, 10:00, 18:45, 22:15)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏î‡∏µ (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î):**
{examples_str}
- "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡∏ß‡∏á 25/03/1998 ‡πÄ‡∏ß‡∏•‡∏≤ 08:30"
- "‡∏£‡∏≤‡∏®‡∏µ‡∏≠‡∏∞‡πÑ‡∏£ 12/07/2005 ‡πÄ‡∏ß‡∏•‡∏≤ 16:45"
- "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤ 30/11/1992 ‡πÄ‡∏ß‡∏•‡∏≤ 20:00"
- "‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤ 05/01/2001 ‡πÄ‡∏ß‡∏•‡∏≤ 12:15"
- "‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 12 ‡∏ö‡πâ‡∏≤‡∏ô 18/06/1997 ‡πÄ‡∏ß‡∏•‡∏≤ 14:30"

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á:**
- "‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?" (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)
- "‡∏£‡∏≤‡∏®‡∏µ‡∏Å‡∏£‡∏Å‡∏é‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)
- "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?" (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (JSON):**
{{
  "questions": [
    "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 1 (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)",
    "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 2 (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î)",
    ...
  ]
}}

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY ‡πÅ‡∏•‡∏∞ HH:MM**

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á {num_questions} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô:"""
        
        openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        
        response = openai_client.chat.completions.create(
            model=openai_model,
            messages=[
                {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,
            max_tokens=1000,
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # ‡∏•‡∏ö markdown code blocks ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        # Parse JSON
        try:
            result = json.loads(response_text)
            questions = result.get("questions", [])
            
            if not isinstance(questions, list):
                questions = [questions]
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
            questions = [q.strip() for q in questions if q and q.strip()]
            
            return questions
            
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è Error parsing JSON response: {e}")
            logger.warning(f"Response text: {response_text[:200]}...")
            return []
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error generating questions: {e}")
        import traceback
        traceback.print_exc()
        return []


def generate_answer_from_retrieved_data(
    question: str,
    retrieved_docs: List[Dict[str, Any]],
    invalid_answer_phrases: List[str] = None
) -> Dict[str, Any]:
    """
    ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
    
    Args:
        question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        retrieved_docs: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤ (‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á contexts)
        invalid_answer_phrases: ‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        
    Returns:
        Dict: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ contexts
    """
    try:
        # Import ask_question_to_rag ‡∏à‡∏≤‡∏Å retrieval_utils
        import sys
        import os
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))
        from app.retrieval_utils import ask_question_to_rag
        
        if not retrieved_docs:
            return {
                "answer": "",
                "contexts": []
            }
        
        # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ (ask_question_to_rag)
        # ‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
        logger.debug(f"üîÑ ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
        answer = ask_question_to_rag(question, user_id="dataset_generation")
        
        if not answer or not answer.strip():
            return {
                "answer": "",
                "contexts": []
            }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if invalid_answer_phrases:
            answer_lower = answer.lower()
            if any(phrase.lower() in answer_lower for phrase in invalid_answer_phrases):
                logger.warning(f"‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {answer[:80]}...")
                return {
                    "answer": "",
                    "contexts": []
                }
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á contexts ‡∏à‡∏≤‡∏Å retrieved_docs
        contexts = []
        for doc in retrieved_docs:
            content = doc.get("content", "")
            if content and content.strip():
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å (500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÄ‡∏õ‡πá‡∏ô context
                contexts.append(content[:500].strip())
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ contexts ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å retrieved docs
        if not contexts:
            contexts = [doc.get("content", "")[:500] for doc in retrieved_docs[:3] if doc.get("content", "").strip()]
        
        # ‡∏Å‡∏£‡∏≠‡∏á contexts ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
        contexts = [ctx.strip() for ctx in contexts if ctx and ctx.strip()]
        
        return {
            "answer": answer.strip(),
            "contexts": contexts
        }
        
    except ImportError as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import ask_question_to_rag ‡πÑ‡∏î‡πâ: {e}")
        logger.error("   ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ app/retrieval_utils.py ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô ask_question_to_rag")
        return {
            "answer": "",
            "contexts": []
        }
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error generating answer with RAG: {e}")
        import traceback
        traceback.print_exc()
        return {
            "answer": "",
            "contexts": []
        }


def calculate_similarity(question: str, content_embeddings: List[float], embedding_model=None) -> float:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏±‡∏ö content ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ embeddings
    
    Args:
        question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        content_embeddings: embeddings ‡∏Ç‡∏≠‡∏á content
        embedding_model: SentenceTransformer model (‡∏ñ‡πâ‡∏≤ None ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
        
    Returns:
        float: similarity score (0-1)
    """
    try:
        from sentence_transformers import SentenceTransformer
        
        # ‡πÇ‡∏´‡∏•‡∏î embedding model ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if embedding_model is None:
            embedding_model = SentenceTransformer("minishlab/potion-multilingual-128M", device="cpu")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        question_embedding = embedding_model.encode(question, convert_to_numpy=True)
        
        # ‡πÉ‡∏ä‡πâ embeddings ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        content_embedding = np.array(content_embeddings)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ dimensions ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        if len(question_embedding) != len(content_embedding):
            logger.warning(f"‚ö†Ô∏è Embedding dimensions ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (question: {len(question_embedding)}, content: {len(content_embedding)})")
            return 0.0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity
        similarity = np.dot(question_embedding, content_embedding) / (
            np.linalg.norm(question_embedding) * np.linalg.norm(content_embedding)
        )
        
        return float(similarity)
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error calculating similarity: {e}")
        return 0.0


def generate_dataset_from_mongo(
    num_questions_batch: int = 10,
    max_total_questions: int = 200,
    top_k_retrieval: int = 5,
    retrieval_threshold: float = 0.10
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ workflow ‡πÉ‡∏´‡∏°‡πà:
    1. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    2. ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB
    3. ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
    
    **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ (ask_question_to_rag) ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏≠‡∏á
    ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
    
    Args:
        num_questions_batch: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πà‡∏≠ batch
        max_total_questions: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        top_k_retrieval: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà retrieve ‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        retrieval_threshold: threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö similarity score
        
    Returns:
        List[Dict]: Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAGAS
    """
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö OpenAI API key
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key or openai_key == "your-openai-api-key-here":
        logger.error("‚ùå OPENAI_API_KEY ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default")
        return []
    
    openai_client = OpenAI(api_key=openai_key)
    
    # ‡πÇ‡∏´‡∏•‡∏î embedding model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö retrieval
    try:
        from sentence_transformers import SentenceTransformer
        embedding_model = SentenceTransformer("minishlab/potion-multilingual-128M", device="cpu")
        logger.info("‚úÖ ‡πÇ‡∏´‡∏•‡∏î embedding model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö retrieval ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÑ‡∏î‡πâ: {e}")
        return []
    
    # ‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    invalid_answer_phrases = [
        "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤",
        "‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ",
        "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤",
        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà", "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ",
        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏®‡∏µ",
        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏•‡∏±‡∏Ñ‡∏ì‡∏≤", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏®‡∏µ"
    ]
    
    dataset = []
    seen_questions = set()  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
    
    logger.info("üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏î‡πâ‡∏ß‡∏¢ workflow ‡πÉ‡∏´‡∏°‡πà:")
    logger.info("   1. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
    logger.info("   2. ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB")
    logger.info("   3. ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤")
    logger.info("="*60)
    
    batch_num = 0
    while len(dataset) < max_total_questions:
        batch_num += 1
        remaining_questions = max_total_questions - len(dataset)
        current_batch_size = min(num_questions_batch, remaining_questions)
        
        logger.info(f"\nüì¶ Batch {batch_num}: ‡∏™‡∏£‡πâ‡∏≤‡∏á {current_batch_size} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        
        # 1. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        logger.info("üîÑ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°...")
        questions = generate_questions_with_llm(
            openai_client=openai_client,
            num_questions=current_batch_size
        )
        
        if not questions:
            logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ - ‡∏Ç‡πâ‡∏≤‡∏° batch ‡∏ô‡∏µ‡πâ")
            continue
        
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ {len(questions)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        
        # 2. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        for i, question in enumerate(questions, 1):
            if len(dataset) >= max_total_questions:
                logger.info(f"‚úÖ ‡∏ñ‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß: {max_total_questions}")
                break
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            question_normalized = question.lower().strip()
            if question_normalized in seen_questions:
                logger.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥: {question[:50]}...")
                continue
            seen_questions.add(question_normalized)
            
            logger.info(f"\nüìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° {i}/{len(questions)}: {question[:60]}...")
            
            # 2. ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB
            logger.info("   üîÑ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB...")
            retrieved_docs = retrieve_from_mongodb_with_embedding(
                question=question,
                embedding_model=embedding_model,
                top_k=top_k_retrieval,
                threshold=retrieval_threshold
            )
            
            if not retrieved_docs:
                logger.warning(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ")
                continue
            
            logger.info(f"   ‚úÖ Retrieve ‡πÑ‡∏î‡πâ {len(retrieved_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
            if retrieved_docs:
                max_similarity = max(doc.get("similarity", 0.0) for doc in retrieved_docs)
                logger.info(f"   üìä Similarity ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {max_similarity:.4f}")
            
            # 3. ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
            logger.info("   üîÑ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG (ask_question_to_rag) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
            answer_result = generate_answer_from_retrieved_data(
                question=question,
                retrieved_docs=retrieved_docs,
                invalid_answer_phrases=invalid_answer_phrases
            )
            
            answer = answer_result.get("answer", "").strip()
            contexts = answer_result.get("contexts", [])
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if not answer:
                logger.warning(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ")
                continue
            
            if any(phrase in answer for phrase in invalid_answer_phrases):
                logger.warning(f"   ‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ")
                continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ contexts ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not contexts or not any(ctx.strip() for ctx in contexts if ctx):
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å retrieved docs ‡πÄ‡∏õ‡πá‡∏ô contexts
                contexts = [doc.get("content", "")[:500] for doc in retrieved_docs[:3] if doc.get("content", "").strip()]
            
            # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤ (‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
            best_doc = retrieved_docs[0] if retrieved_docs else {}
            
            dataset.append({
                "question": question,
                "ground_truth": answer,
                "contexts": contexts,
                "source": best_doc.get("source", "unknown"),
                "content_type": best_doc.get("collection", "unknown").replace("processed_", "").replace("_chunks", ""),
                "page": best_doc.get("page", None),
                "similarity": best_doc.get("similarity", 0.0),
                "_mongodb_id": best_doc.get("_mongodb_id", "unknown"),
            })
            
            logger.info(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(dataset)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°)")
        
        logger.info(f"\n‚úÖ Batch {batch_num} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(dataset)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    invalid_count = 0
    low_similarity_count = 0
    
    for item in dataset:
        answer = str(item.get("ground_truth", "")).strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if any(phrase in answer for phrase in invalid_answer_phrases):
            invalid_count += 1
            similarity = item.get("similarity", 0.0)
            page = item.get("page", "unknown")
            mongo_id = item.get("_mongodb_id", "unknown")
            logger.warning(f"‚ö†Ô∏è ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
            logger.warning(f"   ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {item.get('question', '')[:60]}...")
            logger.warning(f"   ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:80]}...")
            logger.warning(f"   MongoDB: page={page}, _id={mongo_id[:20] if mongo_id != 'unknown' else 'unknown'}, similarity={similarity:.4f}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö similarity ‡∏ï‡πà‡∏≥
        similarity = item.get("similarity", 0.0)
        if similarity < retrieval_threshold:
            low_similarity_count += 1
    
    if invalid_count > 0:
        logger.warning(f"\n‚ö†Ô∏è ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {invalid_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        logger.warning("   ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å")
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å
        dataset = [item for item in dataset if not any(phrase in str(item.get("ground_truth", "")).strip() for phrase in invalid_answer_phrases)]
        logger.info(f"‚úÖ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(dataset)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
    else:
        logger.info(f"‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ similarity
    if low_similarity_count > 0:
        logger.warning(f"\n‚ö†Ô∏è ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å (< {retrieval_threshold}) {low_similarity_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        logger.warning("   ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å MongoDB ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• source
    source_stats = {}
    for item in dataset:
        source = item.get("source", "unknown")
        source_stats[source] = source_stats.get(source, 0) + 1
    
    logger.info(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• source:")
    for source, count in source_stats.items():
        logger.info(f"   {source}: {count} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
    
    logger.info(f"\n‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(dataset)} ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
    return dataset


def save_dataset(dataset: List[Dict[str, Any]], output_file: str = "dataset_from_mongo.json"):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
    
    Args:
        dataset: Dataset ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        output_file: ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    try:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏•‡∏ö source ‡πÅ‡∏•‡∏∞ content_type ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö dataset.json ‡πÄ‡∏î‡∏¥‡∏°)
        output_data = []
        for item in dataset:
            output_data.append({
                "question": item["question"],
                "ground_truth": item["ground_truth"],
                "contexts": item["contexts"],
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á {output_file} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(output_data)} items)")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        if output_data:
            logger.info("\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:")
            for i, item in enumerate(output_data[:3], 1):
                logger.info(f"\n{i}. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {item['question']}")
                logger.info(f"   ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {item['ground_truth'][:100]}...")
        
    except Exception as e:
        logger.error(f"‚ùå Error saving dataset: {e}")
        import traceback
        traceback.print_exc()


def save_dataset_to_google_sheets(
    dataset: List[Dict[str, Any]],
    spreadsheet_id: Optional[str] = None,
    worksheet_name: str = "Dataset"
) -> bool:
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets
    
    Args:
        dataset: Dataset ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        spreadsheet_id: ID ‡∏Ç‡∏≠‡∏á Google Spreadsheet (‡∏ñ‡πâ‡∏≤ None ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å GOOGLE_SHEETS_ID)
        worksheet_name: ‡∏ä‡∏∑‡πà‡∏≠ worksheet ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        
    Returns:
        bool: True ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, False ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    """
    try:
        import gspread
        from google.oauth2.service_account import Credentials
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not dataset or len(dataset) == 0:
            logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (dataset ‡∏ß‡πà‡∏≤‡∏á)")
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö credentials path
        credentials_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH")
        if not credentials_path or not os.path.exists(credentials_path):
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GOOGLE_SHEETS_CREDENTIALS_PATH: {credentials_path}")
            logger.error("   ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå credentials ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡∏∞ path ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
            return False
        
        # ‡πÇ‡∏´‡∏•‡∏î credentials
        creds = Credentials.from_service_account_file(
            credentials_path,
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
        client = gspread.authorize(creds)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö spreadsheet_id
        if spreadsheet_id is None:
            spreadsheet_id = os.getenv("GOOGLE_SHEETS_ID")
        
        if not spreadsheet_id:
            logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GOOGLE_SHEETS_ID ‡πÉ‡∏ô environment variables")
            logger.error("   ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GOOGLE_SHEETS_ID ‡πÉ‡∏ô .env")
            return False
        
        # ‡πÅ‡∏¢‡∏Å Spreadsheet ID ‡∏à‡∏≤‡∏Å URL (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if "/d/" in spreadsheet_id:
            parts = spreadsheet_id.split("/d/")
            if len(parts) > 1:
                spreadsheet_id = parts[1].split("/")[0].split("?")[0].split("#")[0]
        
        logger.info(f"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets: {spreadsheet_id}")
        
        # ‡πÄ‡∏õ‡∏¥‡∏î spreadsheet
        spreadsheet = client.open_by_key(spreadsheet_id)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ worksheet ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            logger.info(f"‚úÖ ‡∏û‡∏ö worksheet: {worksheet_name}")
            # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
            worksheet.clear()
            logger.info("üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô worksheet")
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=1000, cols=10)
            logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á worksheet ‡πÉ‡∏´‡∏°‡πà: {worksheet_name}")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏•‡∏ö Similarity ‡πÅ‡∏•‡∏∞ MongoDB ID ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠)
        headers = [
            "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°",
            "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Ground Truth)",
            "‡∏´‡∏ô‡πâ‡∏≤",
            "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó",
            "Contexts"
        ]
        
        rows = []
        for item in dataset:
            contexts_str = " | ".join(item.get("contexts", []))[:500] if item.get("contexts") else ""
            row = [
                item.get("question", ""),
                item.get("ground_truth", ""),
                item.get("page", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"),
                item.get("content_type", "unknown"),
                contexts_str
            ]
            rows.append(row)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å headers
        worksheet.update(values=[headers], range_name='A1:E1')
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if rows:
            worksheet.update(values=rows, range_name=f'A2:E{len(rows)+1}')
            logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(rows)} rows ‡∏•‡∏á Google Sheets")
        
        # Format header row
        worksheet.format('A1:E1', {
            'backgroundColor': {'red': 0.2, 'green': 0.4, 'blue': 0.8},
            'textFormat': {'bold': True, 'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}}
        })
        
        logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        logger.info(f"üìä Spreadsheet: {spreadsheet.url}")
        return True
        
    except ImportError:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö gspread library")
        logger.error("   ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install gspread google-auth")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error saving to Google Sheets: {e}")
        logger.error("   ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î")
        import traceback
        traceback.print_exc()
        return False


def save_dataset_to_sheet(dataset: List[Dict[str, Any]], output_file: str = "dataset_from_mongo.csv"):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á CSV/Excel sheet ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: question, answer, page, type
    
    Args:
        dataset: Dataset ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        output_file: ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö .csv ‡πÅ‡∏•‡∏∞ .xlsx)
    """
    try:
        import pandas as pd
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sheet (‡∏•‡∏ö Similarity ‡πÅ‡∏•‡∏∞ MongoDB ID ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠)
        sheet_data = []
        for item in dataset:
            sheet_data.append({
                "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°": item.get("question", ""),
                "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö": item.get("ground_truth", ""),
                "‡∏´‡∏ô‡πâ‡∏≤": item.get("page", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"),
                "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó": item.get("content_type", "unknown"),
            })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
        df = pd.DataFrame(sheet_data)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
        if output_file.endswith('.xlsx'):
            df.to_excel(output_file, index=False, engine='openpyxl')
            logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Excel {output_file} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(sheet_data)} rows)")
        else:
            # ‡πÉ‡∏ä‡πâ CSV ‡πÄ‡∏õ‡πá‡∏ô default
            if not output_file.endswith('.csv'):
                output_file = output_file.replace('.json', '.csv')
            df.to_csv(output_file, index=False, encoding='utf-8-sig')  # ‡πÉ‡∏ä‡πâ utf-8-sig ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Excel ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á CSV {output_file} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(sheet_data)} rows)")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        if sheet_data:
            logger.info("\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô sheet:")
            for i, item in enumerate(sheet_data[:3], 1):
                logger.info(f"\n{i}. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {item['‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°']}")
                logger.info(f"   ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {item['‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö'][:100]}...")
                logger.info(f"   ‡∏´‡∏ô‡πâ‡∏≤: {item['‡∏´‡∏ô‡πâ‡∏≤']}, ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {item['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó']}")
        
    except ImportError:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö pandas library. ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install pandas openpyxl")
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV ‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ (‡∏•‡∏ö Similarity ‡πÅ‡∏•‡∏∞ MongoDB ID ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠)
        try:
            import csv
            with open(output_file.replace('.xlsx', '.csv'), 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=['‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°', '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö', '‡∏´‡∏ô‡πâ‡∏≤', '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó'])
                writer.writeheader()
                for item in dataset:
                    writer.writerow({
                        '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°': item.get("question", ""),
                        '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö': item.get("ground_truth", ""),
                        '‡∏´‡∏ô‡πâ‡∏≤': item.get("page", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"),
                        '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó': item.get("content_type", "unknown"),
                    })
            logger.info(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á CSV {output_file.replace('.xlsx', '.csv')} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡πÉ‡∏ä‡πâ csv module)")
        except Exception as e:
            logger.error(f"‚ùå Error saving dataset to CSV: {e}")
            import traceback
            traceback.print_exc()
    except Exception as e:
        logger.error(f"‚ùå Error saving dataset to sheet: {e}")
        import traceback
        traceback.print_exc()


def main():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å - ‡∏™‡∏£‡πâ‡∏≤‡∏á RAGAS Dataset ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ workflow ‡πÉ‡∏´‡∏°‡πà:
    1. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    2. ‡πÉ‡∏ä‡πâ embedding model ‡πÄ‡∏û‡∏∑‡πà‡∏≠ retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MongoDB
    3. ‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà retrieve ‡∏°‡∏≤
    """
    logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á RAGAS Dataset ‡∏î‡πâ‡∏ß‡∏¢ workflow ‡πÉ‡∏´‡∏°‡πà")
    logger.info("üìå Workflow: LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‚Üí Embedding retrieve ‚Üí LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
    logger.info("="*60)
    
    # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
    num_questions_batch = int(os.getenv("NUM_QUESTIONS_BATCH", "10"))
    max_total_questions = int(os.getenv("MAX_TOTAL_QUESTIONS", "200"))
    top_k_retrieval = int(os.getenv("TOP_K_RETRIEVAL", "5"))
    retrieval_threshold = float(os.getenv("RETRIEVAL_THRESHOLD", "0.10"))
    output_file = os.getenv("OUTPUT_DATASET_FILE", "dataset_from_mongo.json")
    output_sheet_file = os.getenv("OUTPUT_SHEET_FILE", "dataset_from_mongo.csv")
    
    logger.info(f"üìä ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:")
    logger.info(f"   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠ batch: {num_questions_batch}")
    logger.info(f"   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {max_total_questions}")
    logger.info(f"   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà retrieve ‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {top_k_retrieval}")
    logger.info(f"   - Threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö similarity: {retrieval_threshold}")
    logger.info(f"   - ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON: {output_file}")
    logger.info(f"   - ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Sheet: {output_sheet_file}")
    logger.info("="*60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset
    dataset = generate_dataset_from_mongo(
        num_questions_batch=num_questions_batch,
        max_total_questions=max_total_questions,
        top_k_retrieval=top_k_retrieval,
        retrieval_threshold=retrieval_threshold
    )
    
    if not dataset:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÑ‡∏î‡πâ")
        return
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡πÄ‡∏õ‡πá‡∏ô JSON
    save_dataset(dataset, output_file)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡πÄ‡∏õ‡πá‡∏ô Sheet (CSV/Excel)
    save_dataset_to_sheet(dataset, output_sheet_file)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
    google_sheets_enabled = os.getenv("GOOGLE_SHEETS_ENABLED", "false").lower() == "true"
    if google_sheets_enabled:
        logger.info("\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets...")
        logger.info(f"   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {len(dataset)} items")
        success = save_dataset_to_google_sheets(dataset, worksheet_name="Dataset")
        if success:
            logger.info("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        else:
            logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å dataset ‡∏•‡∏á Google Sheets ‡πÑ‡∏î‡πâ")
            logger.info("üí° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î error")
    else:
        logger.info("\nüí° ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Google Sheets? ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GOOGLE_SHEETS_ENABLED=true ‡πÉ‡∏ô .env")
    
    logger.info("\n‚úÖ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    logger.info(f"üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON: {output_file}")
    logger.info(f"üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Sheet: {output_sheet_file}")
    logger.info(f"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {len(dataset)}")


if __name__ == "__main__":
    main()

