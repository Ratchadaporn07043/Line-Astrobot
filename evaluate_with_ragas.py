import os
import json
import argparse
from typing import List, Optional

from dotenv import load_dotenv

# RAGAS / evaluation
import pandas as pd
from datasets import Dataset as HFDataset
from ragas import evaluate
# ‡πÉ‡∏ä‡πâ metric ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: answer_relevancy + metrics ‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_precision,
    context_recall,
)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# RAG system
from app.birth_date_parser import BirthDateParser, create_birth_chart_query
from app.retrieval_utils import ask_question_to_rag_for_evaluation


def load_generated_dataset(path: str, limit: Optional[int] = None) -> List[dict]:
    """Load questions/ground truths/contexts from generated_dataset.json.

    Args:
        path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        limit: ‡∏ñ‡πâ‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà N ‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏™ / ‡πÄ‡∏ó‡∏£‡∏ô)
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError("generated_dataset.json ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á objects")
    if limit is not None and limit > 0:
        data = data[:limit]
    return data


def run_rag_inference(dataset: List[dict]) -> pd.DataFrame:
    """Run RAG for each question without follow-up / history.

    - ‡πÉ‡∏ä‡πâ user_id ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (eval_0, eval_1, ...)
      ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (no follow-up, no shared history)
    - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• context ‡∏à‡∏≤‡∏Å dataset ‡∏ï‡∏≠‡∏ô‡∏ñ‡∏≤‡∏° RAG (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Ragas)
    """
    questions: List[str] = []
    answers: List[str] = []
    ground_truths: List[str] = []
    contexts: List[List[str]] = []  # RAGAS ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á list[str]

    for idx, item in enumerate(dataset):
        question = item.get("question", "").strip()
        gt = item.get("ground_truth") or item.get("answer") or ""
        ctx = item.get("context", "")

        if not question:
            continue

        user_id = f"eval_{idx}"  # user ‡πÉ‡∏´‡∏°‡πà‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° -> ‡πÑ‡∏°‡πà‡∏°‡∏µ follow-up history

        print("\n" + "=" * 80)
        print(f"[RAG EVAL] #{idx} question: {question}")
        print("=" * 80)

        try:
            # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô retrieval ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
            # ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ user context
            parser = BirthDateParser()
            birth_info = parser.extract_birth_info(question)
            
            chart_info = None
            if birth_info and birth_info.get('date'):
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤
                chart_info = parser.generate_birth_chart_info(
                    birth_date=birth_info['date'], 
                    birth_time=birth_info.get('time'), 
                    latitude=birth_info.get('latitude', 13.7563),
                    longitude=birth_info.get('longitude', 100.5018)
                )
                
                rag_contexts = [] # Initialize context list
                
                if chart_info:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏î‡∏≤‡∏ß‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå, ‡∏°‡∏∏‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå, ‡∏™‡∏µ‡∏°‡∏á‡∏Ñ‡∏•) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                    specific_keywords = [
                        '‡∏î‡∏≤‡∏ß', '‡∏°‡∏§‡∏ï‡∏¢‡∏π', '‡∏û‡∏§‡∏´‡∏±‡∏™', '‡πÄ‡∏™‡∏≤‡∏£‡πå', '‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£', '‡∏®‡∏∏‡∏Å‡∏£‡πå', '‡∏û‡∏∏‡∏ò', '‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå', '‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå',
                        '‡∏°‡∏∏‡∏°', '‡πÄ‡∏•‡πá‡∏á', '‡∏Å‡∏∏‡∏°', '‡πÇ‡∏¢‡∏Ñ', '‡∏ï‡∏£‡∏µ‡πÇ‡∏Å‡∏ì', '‡∏£‡∏≤‡∏´‡∏π', '‡πÄ‡∏Å‡∏ï‡∏∏', '‡πÅ‡∏ö‡∏Ñ‡∏Ñ‡∏±‡∏™', '‡πÄ‡∏ô‡∏õ‡∏à‡∏π‡∏ô', '‡∏û‡∏•‡∏π‡πÇ‡∏ï',
                        '‡∏™‡∏µ‡∏°‡∏á‡∏Ñ‡∏•', '‡∏™‡∏µ', '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏ö‡∏ö', '‡∏ä‡∏∏‡∏î', 'accessories', '‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö', '‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',
                        '‡∏û‡∏≤‡∏´‡∏ô‡∏∞', '‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á', '‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£',
                        '‡∏û‡∏∑‡πâ‡∏ô‡∏î‡∏ß‡∏á', '‡∏™‡∏±‡∏ï‡∏ß‡πå', '‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á', '‡∏´‡πâ‡∏≤‡∏°', '‡∏Å‡∏≤‡∏•‡∏Å‡∏¥‡∏ì‡∏µ', '‡πÇ‡∏â‡∏•‡∏Å', '‡∏°‡∏á‡∏Ñ‡∏•', '‡∏î‡∏µ', '‡πÄ‡∏™‡∏µ‡∏¢', '‡πÄ‡∏´‡∏°‡∏≤‡∏∞',
                        '‡∏Å‡∏≤‡∏£‡∏á‡∏≤‡∏ô', '‡∏á‡∏≤‡∏ô', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡πÄ‡∏á‡∏¥‡∏ô', '‡πÇ‡∏ä‡∏Ñ‡∏•‡∏≤‡∏†', '‡∏•‡∏á‡∏ó‡∏∏‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å', '‡∏£‡∏±‡∏Å', '‡∏Ñ‡∏π‡πà', '‡πÅ‡∏ü‡∏ô',
                        '‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û', '‡πÇ‡∏£‡∏Ñ', '‡πÄ‡∏à‡πá‡∏ö‡∏õ‡πà‡∏ß‡∏¢', '‡∏ô‡∏¥‡∏™‡∏±‡∏¢', '‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å'
                    ]
                    is_specific_question = any(keyword in question for keyword in specific_keywords)
                    
                    if is_specific_question:
                        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
                        rag_answer, rag_contexts = ask_question_to_rag_for_evaluation(question, provided_chart_info=chart_info)
                    else:
                        # ‡πÉ‡∏ä‡πâ enhanced query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
                        enhanced_query = create_birth_chart_query(chart_info, birth_info)
                        rag_answer, rag_contexts = ask_question_to_rag_for_evaluation(enhanced_query, provided_chart_info=chart_info)
                else:
                    rag_answer = "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏ß‡∏á‡∏ä‡∏∞‡∏ï‡∏≤‡πÑ‡∏î‡πâ"
                    rag_contexts = []
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                rag_answer, rag_contexts = ask_question_to_rag_for_evaluation(question)
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å ask_question_to_rag_for_evaluation: {e}")
            import traceback
            traceback.print_exc()
            rag_answer = ""
            rag_contexts = []

        questions.append(question)
        answers.append(rag_answer or "")
        ground_truths.append(gt or "")
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAGAS ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ context ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (User Request: No Dataset Fallback)
        contexts.append(rag_contexts)

    df = pd.DataFrame(
        {
            "question": questions,
            "answer": answers,
            "ground_truth": ground_truths,
            "contexts": contexts,
        }
    )
    return df


def evaluate_with_ragas_main():
    """Main entrypoint for running RAGAS evaluation.

    ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å generated_dataset.json ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö RAG
    ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ follow-up ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ chat history ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    load_dotenv()

    parser = argparse.ArgumentParser(
        description="Evaluate RAG answers with RAGAS using generated_dataset.json"
    )
    parser.add_argument(
        "--limit",
        "-n",
        type=int,
        default=None,
        help="‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 50). ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡πÉ‡∏ô generated_dataset.json",
    )
    args = parser.parse_args()

    dataset_path = os.path.join(os.path.dirname(__file__), "generated_dataset.json")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå generated_dataset.json ‡∏ó‡∏µ‡πà {dataset_path}")

    print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î dataset ‡∏à‡∏≤‡∏Å {dataset_path}...")
    dataset = load_generated_dataset(dataset_path, limit=args.limit)
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
          f"{' (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ --limit)' if args.limit else ''}")

    # ‡∏£‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà
    print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏±‡∏ô RAG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô RAGAS...")
    df = run_rag_inference(dataset)

    print("‚úÖ RAG Inference completed.", flush=True)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á HuggingFace Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ragas
    print("‚è≥ Converting to HFDataset...", flush=True)
    hf_dataset = HFDataset.from_pandas(df)
    print("‚úÖ HFDataset created.", flush=True)

    print("‚è≥ Importing langchain_openai...", flush=True)
    try:
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        print("‚úÖ langchain_openai imported.", flush=True)
    except Exception as e:
        print(f"‚ùå Error importing langchain_openai: {e}", flush=True)
        raise e

    # üÜï RAGAS 0.4.x Compatibility
    print("‚è≥ Importing ragas wrappers...", flush=True)
    try:
        from ragas.llms import LangchainLLMWrapper
        from ragas.embeddings import LangchainEmbeddingsWrapper
        print("‚úÖ Ragas wrappers imported.", flush=True)
    except ImportError:
        # Use dummy check or fail if not found, but we tested they exist
        print("‚ö†Ô∏è Warning: Could not import Langchain wrappers. Might be on older version?", flush=True)
        LangchainLLMWrapper = None
        LangchainEmbeddingsWrapper = None

# ... (Rest of code until ragas_llm init)

    print("\nüìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ RAGAS (Model: gpt-4o-mini) ...", flush=True)
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î LLM ‡πÅ‡∏•‡∏∞ Embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ragas
    try:
        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
             raise ValueError("OPENAI_API_KEY not found in environment")
             
        _llm = ChatOpenAI(model="gpt-4o-mini", api_key=openai_key)
        _emb = OpenAIEmbeddings(api_key=openai_key)
        
        if LangchainLLMWrapper and LangchainEmbeddingsWrapper:
            ragas_llm = LangchainLLMWrapper(_llm)
            ragas_embeddings = LangchainEmbeddingsWrapper(_emb)
            print("‚úÖ Configured Ragas with LangchainLLMWrapper and LangchainEmbeddingsWrapper")
        else:
            ragas_llm = _llm
            ragas_embeddings = _emb
            print("‚ö†Ô∏è Configured Ragas with raw Langchain objects (Legacy Mode)")
            
    except Exception as e:
        print(f"‚ùå Failed to configure Ragas LLM: {e}")
        raise e

    # üÜï RAGAS 0.4.x Prompt Patching for Thai
    try:
        print("üîß Patching Ragas prompts for Thai language (v0.4.x compatible)...")
        
        # 1. Faithfulness - Statement Generation
        if hasattr(faithfulness, 'statement_generator_prompt'):
            # Set language if supported
            if hasattr(faithfulness.statement_generator_prompt, 'language'):
                faithfulness.statement_generator_prompt.language = "thai"
            
            # Update instruction with specific leniency for Astrology
            faithfulness.statement_generator_prompt.instruction += (
                "\n\nIMPORTANT: The answer is in Thai. Split sentences by meaning."
                "Ignore minor elaborations or flowery language commonly used in astrology."
                "Output strictly as a JSON list of strings."
            )
            print("‚úÖ Patched faithfulness.statement_generator_prompt")

        # 2. Faithfulness - NLI Verification
        if hasattr(faithfulness, 'nli_statements_prompt'):
            if hasattr(faithfulness.nli_statements_prompt, 'language'):
                faithfulness.nli_statements_prompt.language = "thai"
            
            faithfulness.nli_statements_prompt.instruction += (
                "\n\nIMPORTANT: The context and statements are in Thai. Analyze semantic meaning."
                "If the context provides a planetary position (e.g., 'Saturn in Taurus'), "
                "CONSIDER standard astrological interpretations (e.g., patience, caution, financial stress) "
                "as 'consistent' or 'supported' by the context, even if the exact interpretation words are missing."
                "Do NOT penalize for using general astrological knowledge that derives from the retrieved positions."
                "Output valid JSON."
            )
            print("‚úÖ Patched faithfulness.nli_statements_prompt")

        # 3. Answer Relevancy - Question Generation
        if hasattr(answer_relevancy, 'question_generation'):
             if hasattr(answer_relevancy.question_generation, 'language'):
                answer_relevancy.question_generation.language = "thai"
             
             answer_relevancy.question_generation.instruction += (
                 "\n\nIMPORTANT: Generate the question in Thai language. "
                 "The generated question should match the style and vocabulary of the answer. "
                 "The answer often contains detailed astrological advice; ensure the generated question reflects a request for such advice."
                 "Output strictly as valid JSON key 'question'."
             )
             print("‚úÖ Patched answer_relevancy.question_generation")

    except Exception as e:
        print(f"‚ö†Ô∏è Failed to patch Ragas prompts: {e}")

    from ragas.run_config import RunConfig

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ RunConfig
    run_config = RunConfig(

        max_workers=4,
        timeout=180,
        max_retries=10,
        max_wait=60
    )

    print("‚è≥ Starting evaluate()... This might take a while.")
    try:
        result = evaluate(
            dataset=hf_dataset,
            metrics=[
                answer_relevancy,
                faithfulness,
                context_precision,
                context_recall,
            ],
            llm=ragas_llm,
            embeddings=ragas_embeddings,
            run_config=run_config,
        )
        print("‚úÖ evaluate() completed successfully.")
    except Exception as e:
        print(f"‚ùå Error during evaluate(): {e}")
        import traceback
        traceback.print_exc()
        raise e

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    out_csv = os.path.join(os.path.dirname(__file__), "ragas_evaluation_results.csv")
    out_json = os.path.join(os.path.dirname(__file__), "ragas_summary.json")

    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡πÑ‡∏õ‡∏ó‡∏µ‡πà {out_csv}")
    result_df = result.to_pandas()
    result_df.to_csv(out_csv, index=False)

    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡πÑ‡∏õ‡∏ó‡∏µ‡πà {out_json}")
    # summary ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å ragas (‡∏≠‡∏≤‡∏à‡∏°‡∏µ NaN ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì)
    try:
        summary = {metric: float(score) for metric, score in result.items()}
    except AttributeError:
        print("‚ö†Ô∏è result.items() failed, computing summary from pandas df", flush=True)
        summary = result.to_pandas().mean(numeric_only=True).to_dict()

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö JSON: summary + per-example results (‡πÑ‡∏°‡πà‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
    # Filter only numeric columns for metrics to avoid including text columns like 'user_input'
    numeric_cols = result_df.select_dtypes(include=['number']).columns.tolist()
    metric_cols = [c for c in numeric_cols if c not in ("question", "answer", "ground_truth", "contexts")]
    
    detailed_results = []
    for idx, row in result_df.iterrows():
        # ‡∏ó‡∏≥ contexts ‡πÉ‡∏´‡πâ serialize ‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô (list[str])
        # Mapping Ragas v0.2 vs v1.0+ column names
        # Ragas 0.4.x / 1.0+ often uses: user_input, response, reference, retrieved_contexts
        contexts_val = row.get("contexts") or row.get("retrieved_contexts")
        
        raw_ctx = contexts_val if contexts_val is not None else []
        
        if isinstance(raw_ctx, (list, tuple)):
            ctx_serializable = [str(x) for x in raw_ctx]
        else:
            # pandas / ragas ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏≤‡∏à‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô ndarray ‡∏´‡∏£‡∏∑‡∏≠ object ‡∏≠‡∏∑‡πà‡∏ô
            try:
                ctx_list = raw_ctx.tolist()  # type: ignore[attr-defined]
                if isinstance(ctx_list, (list, tuple)):
                    ctx_serializable = [str(x) for x in ctx_list]
                else:
                    ctx_serializable = [str(ctx_list)]
            except Exception:
                ctx_serializable = [str(raw_ctx)] if raw_ctx not in (None, "") else []

        detailed_results.append(
            {
                "index": int(idx),
                "question": row.get("question") or row.get("user_input") or "",
                "ground_truth": row.get("ground_truth") or row.get("reference") or "",
                "answer": row.get("answer") or row.get("response") or "",
                "contexts": ctx_serializable,
                "metrics": {
                    m: float(row[m]) if m in row and pd.notna(row[m]) else None
                    for m in metric_cols
                },
            }
        )

    summary_payload = {
        "summary": summary,
        "results": detailed_results,
    }

    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(summary_payload, f, ensure_ascii=False, indent=2)

    print("\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô RAGAS", flush=True)
    print("‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ (‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢):", flush=True)
    
    # Mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    metric_map = {
        "answer_relevancy": "Answer Relevancy",
        "faithfulness": "Faithfulness",
        "context_precision": "Context Precision",
        "context_recall": "Context Recall",
    }

    for metric, score in summary.items():
        thai_name = metric_map.get(str(metric), str(metric))
        print(f"- {thai_name}: {score:.4f}", flush=True)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ô‡∏±‡∏•‡∏î‡πâ‡∏ß‡∏¢ (‡∏à‡∏≤‡∏Å result_df)
    print("\nüìã ‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á):", flush=True)
    # Use numeric_cols which we defined earlier (ensure it's available or redefine)
    cols = [c for c in result_df.select_dtypes(include=['number']).columns if c not in ("question", "answer", "ground_truth", "contexts")]
    for idx, row in result_df.iterrows():
        q = str(row.get("question", ""))[:60].replace("\n", " ")
        # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
        metrics_str = ", ".join(f"{m}={row[m]:.4f}" for m in cols if m in row and pd.notna(row[m]))
        print(f"[{idx}] {q} ... | {metrics_str}", flush=True)


if __name__ == "__main__":
    evaluate_with_ragas_main()
