import os
import io
import base64
import tempfile
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
from dotenv import load_dotenv
from pymongo import MongoClient
from datetime import datetime
import json
import gc
import psutil
import re
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter


# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° camelot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á
try:
    import camelot
    CAMELOT_AVAILABLE = True
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Camelot ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except ImportError:
    CAMELOT_AVAILABLE = False
    print("‚ö†Ô∏è Camelot ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÉ‡∏ä‡πâ pdfplumber ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏ó‡∏ô")

# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° PyThaiNLP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á OCR
try:
    from pythainlp import word_tokenize
    from pythainlp.spell import correct
    from pythainlp.util import normalize
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î PyThaiNLP ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("‚ö†Ô∏è PyThaiNLP ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏ó‡∏ô")

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ MPS device, PIL.ANTIALIAS ‡πÅ‡∏•‡∏∞ tokenizers parallelism
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î .env
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".env"))
load_dotenv(dotenv_path)

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö
PDF_PATH = "data/attention.pdf"
MONGO_URL = os.getenv("MONGO_URL")
ORIGINAL_DB_NAME = "astrobot_original"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà extract ‡πÅ‡∏•‡πâ‡∏ß

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö - Collection Names
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (ORIGINAL_DB_NAME)
ORIGINAL_TEXT_COLLECTION = "original_text_chunks"
ORIGINAL_IMAGE_COLLECTION = "original_image_chunks"
ORIGINAL_TABLE_COLLECTION = "original_table_chunks"

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á bbox ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
def convert_bbox_to_mongodb_format(bbox):
    """
    ‡πÅ‡∏õ‡∏•‡∏á bbox (pymupdf.Rect, tuple, ‡∏´‡∏£‡∏∑‡∏≠ None) ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
    
    Args:
        bbox: pymupdf.Rect, tuple (x0, y0, x1, y1), ‡∏´‡∏£‡∏∑‡∏≠ None
        
    Returns:
        tuple ‡∏´‡∏£‡∏∑‡∏≠ None: (x0, y0, x1, y1) ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    if bbox is None:
        return None
    
    try:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô pymupdf.Rect object
        if hasattr(bbox, 'x0') and hasattr(bbox, 'y0') and hasattr(bbox, 'x1') and hasattr(bbox, 'y1'):
            return (float(bbox.x0), float(bbox.y0), float(bbox.x1), float(bbox.y1))
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏´‡∏£‡∏∑‡∏≠ list
        elif isinstance(bbox, (tuple, list)) and len(bbox) >= 4:
            return (float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]))
        else:
            return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á bbox: {e}")
        return None

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Chunks
def chunk_text_content(text):
    """
    ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Chunks ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ RecursiveCharacterTextSplitter
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÉ‡∏ä‡πâ separators ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)
    - ‡∏Ç‡∏ô‡∏≤‡∏î chunk 1000 characters, overlap 200
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_text(text)

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory
def check_memory():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory"""
    memory = psutil.virtual_memory()
    print(f"üíæ Memory: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
    if memory.percent > 80:
        print("‚ö†Ô∏è High memory usage, running garbage collection...")
        gc.collect()

# üÜï Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô OCR ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î
COMMON_OCR_CORRECTIONS = {
    '‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì‡πå': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì‡πå': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏ï‡∏•‡∏∏': '‡∏ï‡∏∏‡∏•‡∏¢‡πå',
    '‡∏û‡∏¥‡∏à‡∏¥‡∏Å‡∏¥': '‡∏û‡∏¥‡∏à‡∏¥‡∏Å',
    '‡∏û‡∏¥ ‡∏à‡∏¥ ‡∏Å‡∏¥': '‡∏û‡∏¥‡∏à‡∏¥‡∏Å',
    '‡∏°‡∏ñ‡∏¥ ‡∏ô‡∏∏': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏°‡∏ñ‡∏¥‡∏ô‡∏∏': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏Å‡∏£‡∏Å ‡∏è': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏Å‡∏£‡∏Å‡∏è': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏°‡∏Å‡∏£': '‡∏°‡∏Å‡∏£',
    '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏û‡∏§‡∏©‡∏†': '‡∏û‡∏§‡∏©‡∏†',
    '‡∏Å‡∏£‡∏Å‡∏é': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏ò‡∏ô‡∏π': '‡∏ò‡∏ô‡∏π',
    '‡πÄ‡∏°‡∏©': '‡πÄ‡∏°‡∏©',
}

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
def improve_thai_ocr_text(ocr_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
    - Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå)
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ dictionary (COMMON_OCR_CORRECTIONS)
    - ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (pattern matching)
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    - ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ word tokenizer (newmm engine)
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ spell checker (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢)
    - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    if not PYTHAINLP_AVAILABLE or not ocr_text.strip():
        return ocr_text
    
    try:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        text = ocr_text.strip()
        
        # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô OCR ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô normalize)
        for wrong, correct in COMMON_OCR_CORRECTIONS.items():
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
            text = text.replace(wrong, correct)
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô "‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì‡πå")
            text = text.replace(wrong.replace(' ', ''), correct)
        
        # üÜï ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô "‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì‡πå" -> "‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå")
        # ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)
        
        # Pattern 1: ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß (3 ‡∏Ñ‡∏≥)
        thai_word_pattern_3 = r'([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})'
        matches_3 = list(re.finditer(thai_word_pattern_3, text))
        matches_3.sort(key=lambda m: m.end() - m.start(), reverse=True)
        
        # Pattern 2: ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß (2 ‡∏Ñ‡∏≥)
        thai_word_pattern_2 = r'([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})'
        matches_2 = list(re.finditer(thai_word_pattern_2, text))
        matches_2.sort(key=lambda m: m.end() - m.start(), reverse=True)
        
        replacements = []
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô
        for match in matches_3:
            combined = match.group(1) + match.group(2) + match.group(3)
            if combined in COMMON_OCR_CORRECTIONS:
                replacements.append((match.group(0), COMMON_OCR_CORRECTIONS[combined]))
            elif combined in COMMON_OCR_CORRECTIONS.values():
                replacements.append((match.group(0), combined))
            elif len(combined) >= 4:
                replacements.append((match.group(0), combined))
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Ñ‡∏≥
        for match in matches_2:
            combined = match.group(1) + match.group(2)
            if combined in COMMON_OCR_CORRECTIONS:
                replacements.append((match.group(0), COMMON_OCR_CORRECTIONS[combined]))
            elif combined in COMMON_OCR_CORRECTIONS.values():
                replacements.append((match.group(0), combined))
            elif len(combined) >= 3:
                replacements.append((match.group(0), combined))
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å (‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
        for old, new in reversed(replacements):
            text = text.replace(old, new, 1)  # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        
        # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß)
        for wrong, correct in COMMON_OCR_CORRECTIONS.items():
            text = text.replace(wrong, correct)
        
        # üÜï Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå)
        try:
            text = normalize(text)
        except:
            pass  # ‡∏ñ‡πâ‡∏≤ normalize ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î
        text = re.sub(r'([‡∏Å-‡πô])([A-Za-z])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        text = re.sub(r'([A-Za-z])([‡∏Å-‡πô])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©-‡πÑ‡∏ó‡∏¢
        text = re.sub(r'([‡∏Å-‡πô])([0-9])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        text = re.sub(r'([0-9])([‡∏Å-‡πô])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç-‡πÑ‡∏ó‡∏¢
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
        text = re.sub(r'\s+', ' ', text)
        
        # üÜï ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÉ‡∏ä‡πâ newmm engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
        try:
            words = word_tokenize(text, engine='newmm')
        except Exception as e:
            # Fallback: ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏ñ‡πâ‡∏≤ word_tokenize ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            print(f"   ‚ö†Ô∏è word_tokenize ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}, ‡πÉ‡∏ä‡πâ simple split ‡πÅ‡∏ó‡∏ô")
            words = text.split()
        
        # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP spell checker (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
        corrected_words = []
        for word in words:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢)
            has_thai = bool(re.search(r'[‡∏Å-‡πô]', word))
            
            if has_thai and len(word) > 2:
                # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå)
                is_alpha_only = bool(re.match(r'^[‡∏Å-‡πôa-zA-Z]+$', word))
                
                if is_alpha_only:
                    try:
                        corrected = correct(word)
                        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
                        if corrected and corrected != word:
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏° (‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
                            length_diff = abs(len(corrected) - len(word))
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏Å‡πÜ)
                            has_thai_corrected = bool(re.search(r'[‡∏Å-‡πô]', corrected))
                            
                            if length_diff <= 2 and has_thai_corrected:
                                corrected_words.append(corrected)
                            else:
                                corrected_words.append(word)
                        else:
                            corrected_words.append(word)
                    except Exception as e:
                        # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°
                        corrected_words.append(word)
                else:
                    # ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                    corrected_words.append(word)
            else:
                # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                corrected_words.append(word)
        
        # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        improved_text = ' '.join(corrected_words)
        
        # üÜï ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
        improved_text = re.sub(r'\s+', ' ', improved_text)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
        improved_text = re.sub(r'\s+([,\.;:!?])', r'\1', improved_text)
        improved_text = re.sub(r'([,\.;:!?])\s+', r'\1 ', improved_text)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
        improved_text = improved_text.strip()
        
        return improved_text
        
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢: {e}")
        return ocr_text

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
def improve_thai_table_text(table_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
    - ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ " | ")
    - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ improve_thai_ocr_text() (normalize, spell check, word tokenize)
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)
    - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    if not PYTHAINLP_AVAILABLE or not table_text.strip():
        return table_text
    
    try:
        # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß
        rows = table_text.split('\n')
        improved_rows = []
        
        for row in rows:
            if not row.strip():
                improved_rows.append(row)
                continue
            
            # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ " | " (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á " | " ‡πÅ‡∏•‡∏∞ "|")
            # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ " | " ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
            cells = [cell.strip() for cell in row.split(' | ')]
            improved_cells = []
            
            for cell in cells:
                if cell.strip():
                    # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ improve_thai_ocr_text()
                    # ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ó‡∏≥ normalize, spell check, word tokenize, ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
                    improved_cell = improve_thai_ocr_text(cell.strip())
                    improved_cells.append(improved_cell)
                else:
                    improved_cells.append(cell)
            
            # ‡∏£‡∏ß‡∏°‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß (‡πÉ‡∏ä‡πâ " | " ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô)
            improved_row = ' | '.join(improved_cells)
            improved_rows.append(improved_row)
        
        # ‡∏£‡∏ß‡∏°‡πÅ‡∏ñ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        improved_table = '\n'.join(improved_rows)
        
        # üÜï ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
        improved_table = re.sub(r' +', ' ', improved_table)
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
        improved_table = '\n'.join([row.strip() for row in improved_table.split('\n')])
        
        return improved_table
        
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÑ‡∏ó‡∏¢: {e}")
        return table_text

def get_ocr_reader():
    """‡πÇ‡∏´‡∏•‡∏î OCR reader ‡πÅ‡∏ö‡∏ö lazy loading (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Typhoon OCR)"""
    if not hasattr(get_ocr_reader, 'reader'):
        print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Typhoon OCR...")
        try:
            from typhoon_ocr import ocr_document
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API key
            api_key = os.getenv("TYPHOON_OCR_API_KEY")
            if not api_key:
                error_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö TYPHOON_OCR_API_KEY ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ TYPHOON_OCR_API_KEY ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Typhoon OCR"
                print(f"‚ùå {error_msg}")
                raise ValueError(error_msg)
            
            # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ API key ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)
            print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î TYPHOON_OCR_API_KEY ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {len(api_key)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
            
            get_ocr_reader.ocr_document = ocr_document
            get_ocr_reader.reader = "typhoon_ocr"  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô flag
            print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Typhoon OCR ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        except ImportError as e:
            error_msg = f"‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ Typhoon OCR ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à typhoon-ocr ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"
            print(f"‚ùå {error_msg}")
            raise ImportError(error_msg)
        except ValueError as e:
            raise  # Re-raise ValueError from API key check
        except Exception as e:
            error_msg = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Typhoon OCR ‡πÑ‡∏î‡πâ: {e}"
            print(f"‚ùå {error_msg}")
            raise RuntimeError(error_msg)
    return get_ocr_reader.reader

def perform_ocr_on_image_bytes(image_bytes):
    """
    ‡∏ó‡∏≥ OCR ‡∏ö‡∏ô image bytes ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Typhoon OCR
    
    Args:
        image_bytes: bytes ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        
    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å OCR (‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡πÅ‡∏•‡πâ‡∏ß)
        
    Raises:
        RuntimeError: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ Typhoon OCR ‡πÑ‡∏î‡πâ
    """
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ Typhoon OCR
    reader = get_ocr_reader()
    if reader != "typhoon_ocr" or not hasattr(get_ocr_reader, 'ocr_document'):
        raise RuntimeError("Typhoon OCR ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö TYPHOON_OCR_API_KEY ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á typhoon-ocr")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
            tmp_file.write(image_bytes)
            tmp_path = tmp_file.name
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Typhoon OCR
        ocr_document = get_ocr_reader.ocr_document
        markdown_text = ocr_document(pdf_or_image_path=tmp_path)
        
        # ‡πÅ‡∏õ‡∏•‡∏á markdown ‡πÄ‡∏õ‡πá‡∏ô plain text (‡∏•‡∏ö markdown syntax)
        # ‡∏•‡∏ö markdown headers, bold, italic, etc.
        text = re.sub(r'#+\s*', '', markdown_text)  # ‡∏•‡∏ö headers
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # ‡∏•‡∏ö bold
        text = re.sub(r'\*([^*]+)\*', r'\1', text)  # ‡∏•‡∏ö italic
        text = re.sub(r'`([^`]+)`', r'\1', text)  # ‡∏•‡∏ö code
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)  # ‡∏•‡∏ö links
        text = re.sub(r'\n+', ' ', text)  # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà newlines ‡∏î‡πâ‡∏ß‡∏¢ space
        text = re.sub(r'\s+', ' ', text).strip()  # ‡∏•‡∏ö spaces ‡∏ã‡πâ‡∏≥
        
        # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å OCR
        if text.strip():
            text = improve_thai_ocr_text(text)
        
        return text
    except Exception as e:
        error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Typhoon OCR: {e}"
        print(f"‚ùå {error_msg}")
        raise RuntimeError(error_msg)
    finally:
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading
def get_embedding_model():
    """‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• minishlab/potion-multilingual-128M"""
    if not hasattr(get_embedding_model, 'model'):
        model_name = "minishlab/potion-multilingual-128M"
        print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î embedding model: {model_name}...")
        get_embedding_model.model = SentenceTransformer(model_name, device="cpu")
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î embedding model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_name}")
    return get_embedding_model.model

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
def create_text_embedding(text):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
        
    Returns:
        list: embedding vector (list of floats) ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    """
    if not text or not text.strip():
        return None
    
    try:
        model = get_embedding_model()
        embedding = model.encode(text, convert_to_numpy=True).tolist()
        return embedding
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding: {e}")
        return None

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
def extract_text_with_pymupdf(path):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
    """
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    text_output = ""
    doc = fitz.open(path)
    
    try:
        for page_num, page in enumerate(doc):
            page_text = page.get_text("text")
            if page_text.strip():
                text_output += f"\n--- ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ---\n{page_text}"
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 20 ‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 20 == 0:
                check_memory()
                
    finally:
        doc.close()
    
    return text_output

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory management)
def extract_images_from_page(page_num, pymupdf_page, doc):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
    """
    images_data = []
    
    try:
        images = pymupdf_page.get_images(full=True)
        if images:
            print(f"   üñºÔ∏è ‡∏´‡∏ô‡πâ‡∏≤ {page_num}: ‡∏û‡∏ö {len(images)} ‡∏£‡∏π‡∏õ")
        
        for img_index, img in enumerate(images):
            try:
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                image = Image.open(io.BytesIO(image_bytes))
                width, height = image.size
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                if width * height > 1500000:  # 1.5M pixels
                    print(f"      ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà {img_index + 1} ({width}x{height})")
                    continue
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                if width < 50 or height < 50:
                    print(f"      ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å {img_index + 1} ({width}x{height})")
                    continue
                
                # OCR (‡πÉ‡∏ä‡πâ Typhoon OCR - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡πÅ‡∏•‡πâ‡∏ß)
                improved_text = perform_ocr_on_image_bytes(image_bytes)
                
                if improved_text.strip():
                    image_info = {
                        "page": page_num,
                        "image_index": img_index + 1,
                        "text": improved_text,  # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡πÅ‡∏•‡πâ‡∏ß
                        "improved_text": improved_text,
                        "image_base64": base64.b64encode(image_bytes).decode("utf-8"),
                         "type": "image", # Add type
                         "metadata": {
                            "source": "image_ocr",
                            "page": page_num
                        }
                    }
                    images_data.append(image_info)
                    
                    print(f"      ‚úÖ ‡∏£‡∏π‡∏õ {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (OCR)")
                
                # ‡∏•‡πâ‡∏≤‡∏á memory
                del image, image_bytes
                
            except Exception as e:
                print(f"      ‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_index + 1} ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {e}")
                continue
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
        if len(images_data) > 20:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 20 ‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
            print("      ‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 20 ‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤")
            images_data = images_data[:20]
            
    except Exception as e:
         print(f"   ‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {e}")

    return images_data

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ camelot + PyThaiNLP (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ camelot)
def extract_tables_from_page(path, page_num):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ camelot + PyThaiNLP (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
    """
    tables_data = []
    
    if not CAMELOT_AVAILABLE:
        # Silently fail or simple print if not available, as handled in top level
        # But we already checked imports.
        return []
    
    try:
        # ‡πÉ‡∏ä‡πâ camelot extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å PDF ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        # camelot pages argument accepts strings like '1', '1-5', etc.
        # page_num is 1-based index for camelot
        
        tables = []
        try:
            # Suppress stdout from camelot if possible or just let it print
            tables = camelot.read_pdf(path, pages=str(page_num), flavor='lattice')
        except Exception as e1:
            try:
                tables = camelot.read_pdf(path, pages=str(page_num), flavor='stream')
            except Exception as e2:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà error ‡∏Å‡πá‡πÅ‡∏Ñ‡πà return empty
                pass
        
        if tables and len(tables) > 0:
            print(f"   üìä ‡∏´‡∏ô‡πâ‡∏≤ {page_num}: ‡∏û‡∏ö {len(tables)} ‡∏ï‡∏≤‡∏£‡∏≤‡∏á")

        for table_index, table in enumerate(tables):
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô list of lists
                try:
                    table_data = table.df.values.tolist()
                except:
                    table_data = [[str(cell) for cell in row] for row in table.df.values] if hasattr(table.df, 'values') else []
                
                # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                table_text = ""
                for row in table_data:
                    if row:
                        improved_cells = []
                        for cell in row:
                            cell_str = str(cell).strip() if cell is not None and str(cell).strip() else ""
                            if cell_str:
                                if PYTHAINLP_AVAILABLE:
                                    improved_cell = improve_thai_ocr_text(cell_str)
                                else:
                                    improved_cell = cell_str
                                improved_cells.append(improved_cell)
                            else:
                                improved_cells.append("")
                        
                        row_text = " | ".join(improved_cells)
                        if row_text.strip():
                            table_text += row_text + "\n"
                
                if table_text.strip():
                    improved_table_text = improve_thai_table_text(table_text.strip())
                    
                    table_info = {
                        "page": page_num,
                        "table_index": table_index + 1,
                        "original_text": table_text.strip(),
                        "improved_text": improved_table_text,
                        "text": improved_table_text,
                        "bbox": table._bbox if hasattr(table, '_bbox') else None,
                        "type": "table", # Add type
                        "metadata": {
                            "source": "table_camelot",
                            "page": page_num
                        }
                    }
                    tables_data.append(table_info)
                    print(f"      ‚úÖ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á {table_index + 1}: {len(improved_table_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á {table_index + 1}: {e}")
                continue
                    
    except Exception as e:
        print(f"   ‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {e}")
    
    return tables_data


# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á MongoDB
def store_original_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á ORIGINAL_DB_NAME
    üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ ORIGINAL_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        db_name = ORIGINAL_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Original)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(chunks)} chunks...")
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now()
            
            # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
            text_content = original_chunk.get('text', '')
            if text_content:
                embedding = create_text_embedding(text_content)
                if embedding:
                    original_chunk['embeddings'] = embedding
                else:
                    print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chunk {i+1} ‡πÑ‡∏î‡πâ")
            
            collection.insert_one(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(chunks)} chunks ‡∏•‡∏á {collection_name} (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        client.close()
        
    except Exception as e:
        print(f"‚ùó ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_original_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_original_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        original_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now().isoformat()
            
            original_chunks.append(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_original.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(original_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(original_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON: {e}")

        return text_chunks
        
    except Exception as e:
        print(f"‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: {e}")
        import traceback
        traceback.print_exc()
        return []

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö - ‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
def process_single_page(page_num, pymupdf_page, pdfplumber_pdf, doc_id_counter, pdf_path=None):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: Extract ‚Üí Clean ‚Üí Chunk ‚Üí Store
    üÜï ‡πÉ‡∏ä‡πâ RecursiveCharacterTextSplitter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chunking
    """
    page_results = {
        'has_content': False,
        'text_chunks': [],
        'image_chunks': [],
        'table_chunks': []
    }
    
    try:
        print(f"\n{'='*50}")
        print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}")
        print(f"{'='*50}")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        text = pymupdf_page.get_text("text")
        if not text.strip():
            print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
            return page_results

        # Clean text ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ‡πÉ‡∏ä‡πâ improved_ocr logic ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (optional) ‡πÅ‡∏ï‡πà‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ text ‡∏à‡∏≤‡∏Å PDF ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        # ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏° improve_thai_ocr_text ‡∏ñ‡πâ‡∏≤ PDF text ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ PDF text ‡πÅ‡∏¢‡πà ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ OCR ‡∏´‡∏£‡∏∑‡∏≠ improve logic
        
        # ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á Chunk
        chunks = chunk_text_content(text)
        print(f"   üìù ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ {len(chunks)} chunks")
        
        for i, chunk_text in enumerate(chunks):
             chunk_info = {
                "doc_id": doc_id_counter + i,
                "page": page_num + 1,
                "chunk_index": i + 1,
                "text": chunk_text,
                "type": "text",
                "metadata": {
                    "source": "pdf_text",
                    "page": page_num + 1
                }
            }
             page_results['text_chunks'].append(chunk_info)
        
        page_results['has_content'] = True
        
        # Extract Images
        if pymupdf_page:
            image_chunks = extract_images_from_page(page_num + 1, pymupdf_page, pymupdf_page.parent)
            if image_chunks:
                for i, chunk in enumerate(image_chunks):
                    # Add remaining necessary fields
                    chunk["doc_id"] = doc_id_counter + len(page_results['text_chunks']) + i
                    page_results['image_chunks'].append(chunk)

        # Extract Tables
        if pdf_path and CAMELOT_AVAILABLE:
            table_chunks = extract_tables_from_page(pdf_path, page_num + 1)
            if table_chunks:
                for i, chunk in enumerate(table_chunks):
                    # Add remaining necessary fields
                    chunk["doc_id"] = doc_id_counter + len(page_results['text_chunks']) + len(page_results['image_chunks']) + i
                    page_results['table_chunks'].append(chunk)
        
        return page_results

    except Exception as e:
        print(f"‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: {e}")
        import traceback
        traceback.print_exc()
        return page_results



# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
def store_page_results_to_mongodb(page_results, client, is_first_page=False):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    
    Args:
        page_results: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å process_single_page()
        client: MongoDB client (‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß)
        is_first_page: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πà‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
    """
    try:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° database ‡πÅ‡∏•‡∏∞ collections
        db_original = client[ORIGINAL_DB_NAME]
        
        orig_text_col = db_original[ORIGINAL_TEXT_COLLECTION]
        orig_image_col = db_original[ORIGINAL_IMAGE_COLLECTION]
        orig_table_col = db_original[ORIGINAL_TABLE_COLLECTION]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
        if is_first_page:
            print("üóëÔ∏è ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô MongoDB...")
            orig_text_col.delete_many({})
            orig_image_col.delete_many({})
            orig_table_col.delete_many({})
            print("‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° created_at ‡πÅ‡∏•‡∏∞ embeddings ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å chunk
        now = datetime.now()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Text Chunks
        if page_results['text_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['text_chunks'])} text chunks...")
            for chunk in page_results['text_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_text_col.insert_many(page_results['text_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['text_chunks'])} text chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Image Chunks
        if page_results['image_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['image_chunks'])} image chunks...")
            for chunk in page_results['image_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å OCR)
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö image chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_image_col.insert_many(page_results['image_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['image_chunks'])} image chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Table Chunks
        if page_results['table_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['table_chunks'])} table chunks...")
            for chunk in page_results['table_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö table chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_table_col.insert_many(page_results['table_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['table_chunks'])} table chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        return True
        
    except Exception as e:
        print(f"‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏õ‡∏¢‡∏±‡∏á MongoDB: {e}")
        import traceback
        traceback.print_exc()
        return False

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí loop ‡∏ï‡πà‡∏≠)
def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Pipeline: Extract ‚Üí OCR + PyThaiNLP ‚Üí Store")
    print("üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å MongoDB ‚Üí loop ‡∏ï‡πà‡∏≠")
    print()
    
    client = None
    pymupdf_doc = None
    pdfplumber_pdf = None
    
    try:
        # === INITIALIZATION ===
        print("=== INITIALIZATION ===")
        check_memory()
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏±‡πâ‡∏á PyMuPDF ‡πÅ‡∏•‡∏∞ pdfplumber
        pymupdf_doc = fitz.open(PDF_PATH)
        pdfplumber_pdf = pdfplumber.open(PDF_PATH)
        
        total_pages = len(pymupdf_doc)
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_pages} ‡∏´‡∏ô‡πâ‡∏≤")
        
        # ‡πÄ‡∏õ‡∏¥‡∏î MongoDB connection ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á pipeline)
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        total_text_chunks = 0
        total_image_chunks = 0
        total_table_chunks = 0
        
        doc_id_counter = 1  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
        # === LOOP: More Pages (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤) ===
        print("\n=== STEP 1: PAGE-BY-PAGE PROCESSING & STORING ===")
        for page_num in range(total_pages):
            print(f"\n{'='*60}")
            print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}/{total_pages}")
            print(f"{'='*60}")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Extract)
            page_results = process_single_page(
                page_num=page_num,
                pymupdf_page=pymupdf_doc[page_num],
                pdfplumber_pdf=pdfplumber_pdf,
                doc_id_counter=doc_id_counter,
                pdf_path=PDF_PATH  # üÜï ‡∏™‡πà‡∏á pdf_path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö camelot
            )
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
            is_first_page = (page_num == 0)
            print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡∏•‡∏á MongoDB...")
            
            success = store_page_results_to_mongodb(page_results, client, is_first_page=is_first_page)
            
            if success:
                # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks
                total_text_chunks += len(page_results['text_chunks'])
                total_image_chunks += len(page_results['image_chunks'])
                total_table_chunks += len(page_results['table_chunks'])
                
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            else:
                print(f"‚ö†Ô∏è ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠...")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 ‡∏´‡∏ô‡πâ‡∏≤
            if (page_num + 1) % 5 == 0:
                check_memory()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏´‡∏° (More Pages Decision)
            if page_num < total_pages - 1:
                print(f"‚û°Ô∏è ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å {total_pages - page_num - 1} ‡∏´‡∏ô‡πâ‡∏≤")
            else:
                print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ({total_pages} ‡∏´‡∏ô‡πâ‡∏≤)")
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF
        pymupdf_doc.close()
        pdfplumber_pdf.close()
        pymupdf_doc = None
        pdfplumber_pdf = None
        
        # === ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ===
        print("\n" + "="*60)
        print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        print("="*60)
        print(f"   üìù Text chunks: {total_text_chunks}")
        print(f"   üñºÔ∏è Image chunks: {total_image_chunks}")
        print(f"   üìä Table chunks: {total_table_chunks}")
        print(f"   üìä Total chunks: {total_text_chunks + total_image_chunks + total_table_chunks}")
        
        print("\n‚úÖ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB:")
        print(f"   - Database: {ORIGINAL_DB_NAME}")
        
    except Exception as e:
        print(f"‚ùó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô main pipeline: {e}")
        import traceback
        traceback.print_exc()
        print("üîÑ Running garbage collection...")
        gc.collect()
        check_memory()
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if client:
            try:
                db_original = client[ORIGINAL_DB_NAME]
                
                orig_text_count = db_original[ORIGINAL_TEXT_COLLECTION].count_documents({})
                orig_image_count = db_original[ORIGINAL_IMAGE_COLLECTION].count_documents({})
                orig_table_count = db_original[ORIGINAL_TABLE_COLLECTION].count_documents({})
                
                print(f"\n‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß:")
                print(f"   - Original text chunks: {orig_text_count}")
                print(f"   - Original image chunks: {orig_image_count}")
                print(f"   - Original table chunks: {orig_table_count}")
            except:
                pass
        
    finally:
        # ‡∏õ‡∏¥‡∏î MongoDB connection
        if client:
            try:
                client.close()
                print("üîå ‡∏õ‡∏¥‡∏î MongoDB connection")
            except:
                pass
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î)
        if pymupdf_doc:
            try:
                pymupdf_doc.close()
            except:
                pass
        if pdfplumber_pdf:
            try:
                pdfplumber_pdf.close()
            except:
                pass

if __name__ == "__main__":
    main()
