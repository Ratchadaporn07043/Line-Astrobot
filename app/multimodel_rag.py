import os
import io
import base64
import tempfile
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
from dotenv import load_dotenv
from pymongo import MongoClient
from datetime import datetime
import json
import gc
import psutil
import re
from sentence_transformers import SentenceTransformer

# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° camelot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á
try:
    import camelot
    CAMELOT_AVAILABLE = True
    print("‚úÖ Camelot loaded successfully")
except ImportError:
    CAMELOT_AVAILABLE = False
    print("‚ö†Ô∏è Camelot not available, using pdfplumber for table extraction")

# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° PyThaiNLP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á OCR
try:
    from pythainlp import word_tokenize
    from pythainlp.spell import correct
    from pythainlp.util import normalize
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP loaded successfully")
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("‚ö†Ô∏è PyThaiNLP not available, using basic text processing")

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ MPS device, PIL.ANTIALIAS ‡πÅ‡∏•‡∏∞ tokenizers parallelism
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î .env
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".env"))
load_dotenv(dotenv_path)

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö
PDF_PATH = "data/attention.pdf"
MONGO_URL = os.getenv("MONGO_URL")
ORIGINAL_DB_NAME = "astrobot_original"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà extract ‡πÅ‡∏•‡πâ‡∏ß

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö - Collection Names
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (ORIGINAL_DB_NAME)
ORIGINAL_TEXT_COLLECTION = "original_text_chunks"
ORIGINAL_IMAGE_COLLECTION = "original_image_chunks"
ORIGINAL_TABLE_COLLECTION = "original_table_chunks"

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á bbox ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
def convert_bbox_to_mongodb_format(bbox):
    """
    ‡πÅ‡∏õ‡∏•‡∏á bbox (pymupdf.Rect, tuple, ‡∏´‡∏£‡∏∑‡∏≠ None) ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
    
    Args:
        bbox: pymupdf.Rect, tuple (x0, y0, x1, y1), ‡∏´‡∏£‡∏∑‡∏≠ None
        
    Returns:
        tuple ‡∏´‡∏£‡∏∑‡∏≠ None: (x0, y0, x1, y1) ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    if bbox is None:
        return None
    
    try:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô pymupdf.Rect object
        if hasattr(bbox, 'x0') and hasattr(bbox, 'y0') and hasattr(bbox, 'x1') and hasattr(bbox, 'y1'):
            return (float(bbox.x0), float(bbox.y0), float(bbox.x1), float(bbox.y1))
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏´‡∏£‡∏∑‡∏≠ list
        elif isinstance(bbox, (tuple, list)) and len(bbox) >= 4:
            return (float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]))
        else:
            return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error converting bbox: {e}")
        return None

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory
def check_memory():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory"""
    memory = psutil.virtual_memory()
    print(f"üíæ Memory: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
    if memory.percent > 80:
        print("‚ö†Ô∏è High memory usage, running garbage collection...")
        gc.collect()

# üÜï Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô OCR ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î
COMMON_OCR_CORRECTIONS = {
    '‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì‡πå': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì‡πå': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì': '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå',
    '‡∏ï‡∏•‡∏∏': '‡∏ï‡∏∏‡∏•‡∏¢‡πå',
    '‡∏û‡∏¥‡∏à‡∏¥‡∏Å‡∏¥': '‡∏û‡∏¥‡∏à‡∏¥‡∏Å',
    '‡∏û‡∏¥ ‡∏à‡∏¥ ‡∏Å‡∏¥': '‡∏û‡∏¥‡∏à‡∏¥‡∏Å',
    '‡∏°‡∏ñ‡∏¥ ‡∏ô‡∏∏': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏°‡∏ñ‡∏¥‡∏ô‡∏∏': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏Å‡∏£‡∏Å ‡∏è': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏Å‡∏£‡∏Å‡∏è': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏°‡∏Å‡∏£': '‡∏°‡∏Å‡∏£',
    '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô',
    '‡∏û‡∏§‡∏©‡∏†': '‡∏û‡∏§‡∏©‡∏†',
    '‡∏Å‡∏£‡∏Å‡∏é': '‡∏Å‡∏£‡∏Å‡∏é',
    '‡∏ò‡∏ô‡∏π': '‡∏ò‡∏ô‡∏π',
    '‡πÄ‡∏°‡∏©': '‡πÄ‡∏°‡∏©',
}

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
def improve_thai_ocr_text(ocr_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
    - Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ dictionary
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ spell checker
    - ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ word tokenizer
    """
    if not PYTHAINLP_AVAILABLE or not ocr_text.strip():
        return ocr_text
    
    try:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        text = ocr_text.strip()
        
        # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô OCR ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ú‡∏¥‡∏î (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô normalize)
        for wrong, correct in COMMON_OCR_CORRECTIONS.items():
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
            text = text.replace(wrong, correct)
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô "‡∏™‡∏ç‡∏•‡∏Å‡∏©‡∏ì‡πå")
            text = text.replace(wrong.replace(' ', ''), correct)
        
        # üÜï ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô "‡∏™‡∏ç ‡∏•‡∏Å ‡∏©‡∏ì‡πå" -> "‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå")
        # ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)
        
        # Pattern 1: ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß (3 ‡∏Ñ‡∏≥)
        thai_word_pattern_3 = r'([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})'
        matches_3 = list(re.finditer(thai_word_pattern_3, text))
        matches_3.sort(key=lambda m: m.end() - m.start(), reverse=True)
        
        # Pattern 2: ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß + ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á + ‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ 1-3 ‡∏ï‡∏±‡∏ß (2 ‡∏Ñ‡∏≥)
        thai_word_pattern_2 = r'([‡∏Å-‡πô]{1,3})\s+([‡∏Å-‡πô]{1,3})'
        matches_2 = list(re.finditer(thai_word_pattern_2, text))
        matches_2.sort(key=lambda m: m.end() - m.start(), reverse=True)
        
        replacements = []
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô
        for match in matches_3:
            combined = match.group(1) + match.group(2) + match.group(3)
            if combined in COMMON_OCR_CORRECTIONS:
                replacements.append((match.group(0), COMMON_OCR_CORRECTIONS[combined]))
            elif combined in COMMON_OCR_CORRECTIONS.values():
                replacements.append((match.group(0), combined))
            elif len(combined) >= 4:
                replacements.append((match.group(0), combined))
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Ñ‡∏≥
        for match in matches_2:
            combined = match.group(1) + match.group(2)
            if combined in COMMON_OCR_CORRECTIONS:
                replacements.append((match.group(0), COMMON_OCR_CORRECTIONS[combined]))
            elif combined in COMMON_OCR_CORRECTIONS.values():
                replacements.append((match.group(0), combined))
            elif len(combined) >= 3:
                replacements.append((match.group(0), combined))
        
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å (‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
        for old, new in reversed(replacements):
            text = text.replace(old, new, 1)  # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        
        # üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß)
        for wrong, correct in COMMON_OCR_CORRECTIONS.items():
            text = text.replace(wrong, correct)
        
        # üÜï Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå)
        try:
            text = normalize(text)
        except:
            pass  # ‡∏ñ‡πâ‡∏≤ normalize ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î
        text = re.sub(r'([‡∏Å-‡πô])([A-Za-z])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        text = re.sub(r'([A-Za-z])([‡∏Å-‡πô])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©-‡πÑ‡∏ó‡∏¢
        text = re.sub(r'([‡∏Å-‡πô])([0-9])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        text = re.sub(r'([0-9])([‡∏Å-‡πô])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç-‡πÑ‡∏ó‡∏¢
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
        text = re.sub(r'\s+', ' ', text)
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡πÉ‡∏ä‡πâ newmm engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
        try:
            words = word_tokenize(text, engine='newmm')
        except:
            # Fallback: ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏ñ‡πâ‡∏≤ word_tokenize ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            words = text.split()
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP spell checker
        corrected_words = []
        for word in words:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢)
            has_thai = bool(re.search(r'[‡∏Å-‡πô]', word))
            
            if has_thai and len(word) > 2 and word.isalpha():
                # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                try:
                    corrected = correct(word)
                    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if corrected and corrected != word:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°
                        if abs(len(corrected) - len(word)) <= 2:
                            corrected_words.append(corrected)
                        else:
                            corrected_words.append(word)
                    else:
                        corrected_words.append(word)
                except Exception as e:
                    # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°
                    corrected_words.append(word)
            else:
                # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                corrected_words.append(word)
        
        # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        improved_text = ' '.join(corrected_words)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        improved_text = re.sub(r'\s+', ' ', improved_text).strip()
        
        return improved_text
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in Thai text improvement: {e}")
        return ocr_text

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
def improve_thai_table_text(table_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
    - ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ " | ")
    - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏ñ‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)
    """
    if not PYTHAINLP_AVAILABLE or not table_text.strip():
        return table_text
    
    try:
        # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß
        rows = table_text.split('\n')
        improved_rows = []
        
        for row in rows:
            if not row.strip():
                improved_rows.append(row)
                continue
            
            # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ " | "
            cells = row.split(' | ')
            improved_cells = []
            
            for cell in cells:
                if cell.strip():
                    # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå
                    improved_cell = improve_thai_ocr_text(cell.strip())
                    improved_cells.append(improved_cell)
                else:
                    improved_cells.append(cell)
            
            # ‡∏£‡∏ß‡∏°‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß
            improved_row = ' | '.join(improved_cells)
            improved_rows.append(improved_row)
        
        # ‡∏£‡∏ß‡∏°‡πÅ‡∏ñ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        improved_table = '\n'.join(improved_rows)
        
        return improved_table
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in Thai table text improvement: {e}")
        return table_text

def get_ocr_reader():
    """‡πÇ‡∏´‡∏•‡∏î OCR reader ‡πÅ‡∏ö‡∏ö lazy loading (‡πÉ‡∏ä‡πâ Typhoon OCR)"""
    if not hasattr(get_ocr_reader, 'reader'):
        print("üîÑ Loading Typhoon OCR...")
        try:
            from typhoon_ocr import ocr_document
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API key
            api_key = os.getenv("TYPHOON_OCR_API_KEY")
            if not api_key:
                print("‚ö†Ô∏è TYPHOON_OCR_API_KEY not found in environment variables")
                print("   Falling back to EasyOCR. Set TYPHOON_OCR_API_KEY to use Typhoon OCR")
                raise ValueError("API key not found")
            
            # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ API key ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)
            print(f"‚úÖ TYPHOON_OCR_API_KEY loaded (length: {len(api_key)} characters)")
            
            get_ocr_reader.ocr_document = ocr_document
            get_ocr_reader.reader = "typhoon_ocr"  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô flag
            print("‚úÖ Typhoon OCR loaded successfully")
        except (ImportError, ValueError) as e:
            print(f"‚ö†Ô∏è Typhoon OCR not available ({e}), falling back to EasyOCR")
            import easyocr
            get_ocr_reader.reader = easyocr.Reader(['en', 'th'], gpu=False, verbose=False)
            get_ocr_reader.ocr_document = None
    return get_ocr_reader.reader

def perform_ocr_on_image_bytes(image_bytes):
    """
    ‡∏ó‡∏≥ OCR ‡∏ö‡∏ô image bytes ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Typhoon OCR ‡∏´‡∏£‡∏∑‡∏≠ EasyOCR (fallback)
    
    Args:
        image_bytes: bytes ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        
    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å OCR
    """
    reader = get_ocr_reader()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ Typhoon OCR ‡∏´‡∏£‡∏∑‡∏≠ EasyOCR
    if reader == "typhoon_ocr" and hasattr(get_ocr_reader, 'ocr_document'):
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp_file:
                tmp_file.write(image_bytes)
                tmp_path = tmp_file.name
            
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Typhoon OCR
                ocr_document = get_ocr_reader.ocr_document
                markdown_text = ocr_document(pdf_or_image_path=tmp_path)
                
                # ‡πÅ‡∏õ‡∏•‡∏á markdown ‡πÄ‡∏õ‡πá‡∏ô plain text (‡∏•‡∏ö markdown syntax)
                # ‡∏•‡∏ö markdown headers, bold, italic, etc.
                text = re.sub(r'#+\s*', '', markdown_text)  # ‡∏•‡∏ö headers
                text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # ‡∏•‡∏ö bold
                text = re.sub(r'\*([^*]+)\*', r'\1', text)  # ‡∏•‡∏ö italic
                text = re.sub(r'`([^`]+)`', r'\1', text)  # ‡∏•‡∏ö code
                text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)  # ‡∏•‡∏ö links
                text = re.sub(r'\n+', ' ', text)  # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà newlines ‡∏î‡πâ‡∏ß‡∏¢ space
                text = re.sub(r'\s+', ' ', text).strip()  # ‡∏•‡∏ö spaces ‡∏ã‡πâ‡∏≥
                
                return text
            finally:
                # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
        except Exception as e:
            print(f"‚ö†Ô∏è Error using Typhoon OCR: {e}, falling back to EasyOCR")
            # Fallback to EasyOCR
            import easyocr
            easyocr_reader = easyocr.Reader(['en', 'th'], gpu=False, verbose=False)
            ocr_results = easyocr_reader.readtext(image_bytes)
            ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])
            return ocr_text
    else:
        # ‡πÉ‡∏ä‡πâ EasyOCR (fallback)
        ocr_results = reader.readtext(image_bytes)
        ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])
        return ocr_text

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading
def get_embedding_model():
    """‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_embedding_model, 'model'):
        print("üîÑ Loading embedding model...")
        get_embedding_model.model = SentenceTransformer("minishlab/potion-multilingual-128M", device="cpu")
        print("‚úÖ Embedding model loaded successfully")
    return get_embedding_model.model

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
def create_text_embedding(text):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
        
    Returns:
        list: embedding vector (list of floats) ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    """
    if not text or not text.strip():
        return None
    
    try:
        model = get_embedding_model()
        embedding = model.encode(text, convert_to_numpy=True).tolist()
        return embedding
    except Exception as e:
        print(f"‚ö†Ô∏è Error creating embedding: {e}")
        return None

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
def extract_text_with_pymupdf(path):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
    """
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    text_output = ""
    doc = fitz.open(path)
    
    try:
        for page_num, page in enumerate(doc):
            page_text = page.get_text("text")
            if page_text.strip():
                text_output += f"\n--- ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ---\n{page_text}"
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 20 ‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 20 == 0:
                check_memory()
                
    finally:
        doc.close()
    
    return text_output

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory management)
def extract_images_with_ocr(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP
    """
    print(f"üñºÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    images_data = []
    doc = fitz.open(path)
    
    try:
        for page_num, page in enumerate(doc):
            images = page.get_images(full=True)
            print(f"‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: {len(images)} ‡∏£‡∏π‡∏õ")
            
            for img_index, img in enumerate(images):
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    image = Image.open(io.BytesIO(image_bytes))
                    width, height = image.size
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width * height > 1500000:  # 1.5M pixels
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà {img_index + 1} ({width}x{height})")
                        continue
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width < 50 or height < 50:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å {img_index + 1} ({width}x{height})")
                        continue
                    
                    # OCR (‡πÉ‡∏ä‡πâ Typhoon OCR)
                    ocr_text = perform_ocr_on_image_bytes(image_bytes)
                    
                    if ocr_text.strip():
                        # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                        improved_text = improve_thai_ocr_text(ocr_text)
                        
                        image_info = {
                            "page": page_num + 1,
                            "image_index": img_index + 1,
                            "original_text": ocr_text.strip(),
                            "improved_text": improved_text,
                            "text": improved_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                            "image_base64": base64.b64encode(image_bytes).decode("utf-8")
                        }
                        images_data.append(image_info)
                        
                        print(f"‚úÖ ‡∏£‡∏π‡∏õ {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                    
                    # ‡∏•‡πâ‡∏≤‡∏á memory
                    del image, image_bytes
                    
                except Exception as e:
                    print(f"‚ùó Error processing image {img_index + 1} on page {page_num + 1}: {e}")
                    continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 5 == 0:
                check_memory()
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
            if len(images_data) > 50:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 50 ‡∏£‡∏π‡∏õ
                print("‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 50 ‡∏£‡∏π‡∏õ")
                break
                
    finally:
        doc.close()
    
    return images_data

# üÜï ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ camelot + PyThaiNLP
def extract_tables_with_camelot(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ camelot + PyThaiNLP
    - ‡πÉ‡∏ä‡πâ camelot ‡πÄ‡∏û‡∏∑‡πà‡∏≠ extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤ pdfplumber)
    - ‡πÉ‡∏ä‡πâ PyThaiNLP ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    """
    print(f"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ Camelot ‡∏à‡∏≤‡∏Å: {path}")
    tables_data = []
    
    if not CAMELOT_AVAILABLE:
        print("‚ö†Ô∏è Camelot not available, falling back to pdfplumber")
        return extract_tables_with_pdfplumber(path)
    
    try:
        # ‡πÉ‡∏ä‡πâ camelot extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å PDF
        # flavor='lattice' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö, 'stream' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏Ç‡∏≠‡∏ö
        print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ Camelot...")
        tables = camelot.read_pdf(path, pages='all', flavor='lattice')
        
        print(f"‚úÖ ‡∏û‡∏ö {len(tables)} ‡∏ï‡∏≤‡∏£‡∏≤‡∏á")
        
        for table_index, table in enumerate(tables):
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô list of lists (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ pandas)
                # camelot table.df ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ .values.tolist()
                try:
                    # camelot ‡πÉ‡∏ä‡πâ pandas DataFrame ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á import pandas
                    table_data = table.df.values.tolist()
                except:
                    # Fallback: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ
                    table_data = [[str(cell) for cell in row] for row in table.df.values] if hasattr(table.df, 'values') else []
                
                # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                table_text = ""
                for row in table_data:
                    if row:
                        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå
                        improved_cells = []
                        for cell in row:
                            cell_str = str(cell).strip() if cell is not None and str(cell).strip() else ""
                            if cell_str and PYTHAINLP_AVAILABLE:
                                improved_cell = improve_thai_ocr_text(cell_str)
                                improved_cells.append(improved_cell)
                            else:
                                improved_cells.append(cell_str)
                        
                        row_text = " | ".join(improved_cells)
                        if row_text.strip():
                            table_text += row_text + "\n"
                
                if table_text.strip():
                    # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á)
                    improved_table_text = improve_thai_table_text(table_text.strip())
                    
                    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤ (camelot ‡πÉ‡∏ä‡πâ 1-based page numbers)
                    page_num = table.page if hasattr(table, 'page') else table_index + 1
                    
                    table_info = {
                        "page": page_num,
                        "table_index": table_index + 1,
                        "original_text": table_text.strip(),
                        "improved_text": improved_table_text,
                        "text": improved_table_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        "accuracy": float(table.accuracy) if hasattr(table, 'accuracy') else None,
                        "bbox": table._bbox if hasattr(table, '_bbox') else None
                    }
                    tables_data.append(table_info)
                    print(f"   ‚úÖ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á {table_index + 1} (‡∏´‡∏ô‡πâ‡∏≤ {page_num}): {len(improved_table_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error processing table {table_index + 1}: {e}")
                continue
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory
        check_memory()
                    
    except Exception as e:
        print(f"‚ùó Error extracting tables with Camelot: {e}")
        print("   Falling back to pdfplumber...")
        return extract_tables_with_pdfplumber(path)
    
    return tables_data

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber (fallback)
def extract_tables_with_pdfplumber(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber (fallback)
    """
    print(f"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber ‡∏à‡∏≤‡∏Å: {path}")
    tables_data = []
    
    try:
        with pdfplumber.open(path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                for table_index, table in enumerate(tables):
                    if table:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                        table_text = ""
                        for row in table:
                            if row:
                                row_text = " | ".join([cell if cell else "" for cell in row])
                                table_text += row_text + "\n"
                        
                        if table_text.strip():
                            # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                            improved_table_text = improve_thai_table_text(table_text.strip())
                            
                            table_info = {
                                "page": page_num + 1,
                                "table_index": table_index + 1,
                                "original_text": table_text.strip(),
                                "improved_text": improved_table_text,
                                "text": improved_table_text  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                            }
                            tables_data.append(table_info)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 10 ‡∏´‡∏ô‡πâ‡∏≤
                if page_num % 10 == 0:
                    check_memory()
                    
    except Exception as e:
        print(f"‚ùó Error extracting tables: {e}")
    
    return tables_data

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á MongoDB
def store_original_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á ORIGINAL_DB_NAME
    üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ ORIGINAL_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        db_name = ORIGINAL_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Original)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(chunks)} chunks...")
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now()
            
            # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
            text_content = original_chunk.get('text', '')
            if text_content:
                embedding = create_text_embedding(text_content)
                if embedding:
                    original_chunk['embeddings'] = embedding
                else:
                    print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chunk {i+1} ‡πÑ‡∏î‡πâ")
            
            collection.insert_one(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(chunks)} chunks ‡∏•‡∏á {collection_name} (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        client.close()
        
    except Exception as e:
        print(f"‚ùó MongoDB Atlas connection failed: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_original_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_original_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        original_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now().isoformat()
            
            original_chunks.append(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_original.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(original_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(original_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó Error saving original data to JSON: {e}")

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö - ‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
def process_single_page(page_num, pymupdf_page, pdfplumber_pdf, doc_id_counter, pdf_path=None):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: Extract ‚Üí Store
    üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô) - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° y-coordinate
    
    Args:
        page_num: ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0-based)
        pymupdf_page: ‡∏´‡∏ô‡πâ‡∏≤ PDF ‡∏à‡∏≤‡∏Å PyMuPDF
        pdfplumber_pdf: PDF object ‡∏à‡∏≤‡∏Å pdfplumber (fallback)
        doc_id_counter: counter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        pdf_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå PDF (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö camelot)
        
    Returns:
        dict: {
            'has_content': bool,  # ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤)
            'text_chunks': list,
            'image_chunks': list,
            'table_chunks': list
        }
    """
    page_results = {
        'has_content': False,
        'text_chunks': [],
        'image_chunks': [],
        'table_chunks': []
    }
    
    try:
        print(f"\n{'='*50}")
        print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠)")
        print(f"{'='*50}")
        
        # === STEP 1: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° elements ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ===
        elements = []  # ‡πÄ‡∏Å‡πá‡∏ö elements ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á y-coordinate
        
        # 1.1 ‡∏î‡∏∂‡∏á Text Blocks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        text_blocks = pymupdf_page.get_text("blocks")  # Returns: [(x0, y0, x1, y1, text, block_no, block_type), ...]
        for block in text_blocks:
            if block[6] == 0:  # block_type = 0 ‡∏Ñ‡∏∑‡∏≠ text block
                x0, y0, x1, y1, text, block_no, block_type = block
                if text.strip():
                    elements.append({
                        'type': 'text',
                        'y_pos': y0,  # ‡πÉ‡∏ä‡πâ y0 (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ö‡∏ô‡∏™‡∏∏‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                        'data': {
                            'text': text.strip(),
                            'bbox': (x0, y0, x1, y1),
                            'block_no': block_no
                        }
                    })
        
        # 1.2 ‡∏î‡∏∂‡∏á Images ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        images = pymupdf_page.get_images(full=True)
        if images:
            print(f"   üñºÔ∏è ‡∏û‡∏ö {len(images)} ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ")
        
        for img_index, img in enumerate(images):
            xref = img[0]
            try:
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ bbox ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å get_image_rects
                y_pos = 0  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                bbox = None
                try:
                    from pymupdf.utils import get_image_rects
                    image_rects = get_image_rects(pymupdf_page, xref)
                    if image_rects:
                        bbox = image_rects[0]  # ‡πÉ‡∏ä‡πâ rect ‡πÅ‡∏£‡∏Å
                        if hasattr(bbox, 'y0'):
                            y_pos = bbox.y0
                        elif isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                            y_pos = bbox[1]  # y0
                except Exception as rect_error:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å image list position
                    # (‡∏£‡∏π‡∏õ‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ö‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏ß‡πà‡∏≤)
                    y_pos = img_index * 100  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                
                elements.append({
                    'type': 'image',
                    'y_pos': y_pos,
                    'data': {
                        'xref': xref,
                        'image_index': img_index,
                        'bbox': bbox
                    }
                })
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏£‡∏π‡∏õ {img_index + 1} ‡πÑ‡∏î‡πâ: {e}")
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 0 (‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î)
                elements.append({
                    'type': 'image',
                    'y_pos': img_index * 100,  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                    'data': {
                        'xref': xref,
                        'image_index': img_index,
                        'bbox': None
                    }
                })
        
        # 1.3 ‡∏î‡∏∂‡∏á Tables ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡πÉ‡∏ä‡πâ camelot ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, fallback ‡πÄ‡∏õ‡πá‡∏ô pdfplumber)
        tables_found = False
        
        # üÜï ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ camelot ‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if CAMELOT_AVAILABLE and pdf_path:
            try:
                # ‡πÉ‡∏ä‡πâ camelot extract ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (camelot ‡πÉ‡∏ä‡πâ 1-based page numbers)
                camelot_tables = camelot.read_pdf(pdf_path, pages=str(page_num + 1), flavor='lattice')
                
                if len(camelot_tables) > 0:
                    print(f"   üìä ‡∏û‡∏ö {len(camelot_tables)} ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ Camelot")
                    tables_found = True
                    
                    for table_index, table in enumerate(camelot_tables):
                        try:
                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô list of lists (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ pandas)
                            # camelot table.df ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ .values.tolist()
                            try:
                                # camelot ‡πÉ‡∏ä‡πâ pandas DataFrame ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á import pandas
                                table_data = table.df.values.tolist()
                            except:
                                # Fallback: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ
                                table_data = [[str(cell) for cell in row] for row in table.df.values] if hasattr(table.df, 'values') else []
                            
                            # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                            table_text = ""
                            for row in table_data:
                                if row:
                                    # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå
                                    improved_cells = []
                                    for cell in row:
                                        cell_str = str(cell).strip() if cell is not None and str(cell).strip() else ""
                                        if cell_str and PYTHAINLP_AVAILABLE:
                                            improved_cell = improve_thai_ocr_text(cell_str)
                                            improved_cells.append(improved_cell)
                                        else:
                                            improved_cells.append(cell_str)
                                    
                                    row_text = " | ".join(improved_cells)
                                    if row_text.strip():
                                        table_text += row_text + "\n"
                            
                            if table_text.strip():
                                # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                                improved_table_text = improve_thai_table_text(table_text.strip())
                                
                                # ‡∏î‡∏∂‡∏á bbox ‡∏à‡∏≤‡∏Å camelot
                                bbox = table._bbox if hasattr(table, '_bbox') else None
                                y_pos = bbox[1] if bbox and isinstance(bbox, (list, tuple)) and len(bbox) >= 2 else 500 + (table_index * 150)
                                
                                elements.append({
                                    'type': 'table',
                                    'y_pos': y_pos,
                                    'data': {
                                        'table_index': table_index,
                                        'text': improved_table_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                                        'original_text': table_text.strip(),
                                        'improved_text': improved_table_text,
                                        'bbox': bbox
                                    }
                                })
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è Error processing camelot table {table_index + 1}: {e}")
                            continue
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error using Camelot: {e}, falling back to pdfplumber")
                tables_found = False
        
        # Fallback: ‡πÉ‡∏ä‡πâ pdfplumber ‡∏ñ‡πâ‡∏≤ camelot ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ
        if not tables_found and page_num < len(pdfplumber_pdf.pages):
            pdfplumber_page = pdfplumber_pdf.pages[page_num]
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ bbox ‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            try:
                # ‡πÉ‡∏ä‡πâ find_tables ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ bbox (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                if hasattr(pdfplumber_page, 'find_tables'):
                    table_objects = pdfplumber_page.find_tables()
                    
                    for table_index, table_obj in enumerate(table_objects):
                        if table_obj and hasattr(table_obj, 'bbox'):
                            bbox = table_obj.bbox
                            y_pos = bbox[1] if isinstance(bbox, (list, tuple)) else getattr(bbox, 'y0', bbox[1])
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                            table = table_obj.extract() if hasattr(table_obj, 'extract') else None
                            table_text = ""
                            if table:
                                for row in table:
                                    if row:
                                        row_text = " | ".join([cell if cell else "" for cell in row])
                                        table_text += row_text + "\n"
                            
                            if table_text.strip():
                                # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                                improved_table_text = improve_thai_table_text(table_text.strip())
                                
                                elements.append({
                                    'type': 'table',
                                    'y_pos': y_pos,
                                    'data': {
                                        'table_index': table_index,
                                        'text': improved_table_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                                        'original_text': table_text.strip(),
                                        'improved_text': improved_table_text,
                                        'bbox': bbox
                                    }
                                })
                else:
                    raise AttributeError("find_tables not available")
            except Exception as e:
                # Fallback: ‡∏ñ‡πâ‡∏≤ find_tables ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ extract_tables ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ find_tables ‡πÑ‡∏î‡πâ: {e}, ‡πÉ‡∏ä‡πâ extract_tables ‡πÅ‡∏ó‡∏ô")
                tables = pdfplumber_page.extract_tables()
                
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á text ‡πÅ‡∏•‡∏∞ image elements ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                existing_y_positions = [e['y_pos'] for e in elements]
                base_y_pos = max(existing_y_positions) if existing_y_positions else 500  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 500 ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ elements ‡∏≠‡∏∑‡πà‡∏ô
                
                for table_index, table in enumerate(tables):
                    if table:
                        table_text = ""
                        for row in table:
                            if row:
                                row_text = " | ".join([cell if cell else "" for cell in row])
                                table_text += row_text + "\n"
                        
                        if table_text.strip():
                            # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                            improved_table_text = improve_thai_table_text(table_text.strip())
                            
                            # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å elements ‡∏≠‡∏∑‡πà‡∏ô‡πÜ)
                            table_y_pos = base_y_pos + (table_index * 150)
                            elements.append({
                                'type': 'table',
                                'y_pos': table_y_pos,
                                'data': {
                                    'table_index': table_index,
                                    'text': improved_table_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                                    'original_text': table_text.strip(),
                                    'improved_text': improved_table_text,
                                    'bbox': None
                                }
                            })
        
        # === STEP 2: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö elements ‡∏ï‡∏≤‡∏° y-coordinate (‡∏à‡∏≤‡∏Å‡∏ö‡∏ô‡∏•‡∏á‡∏•‡πà‡∏≤‡∏á) ===
        elements.sort(key=lambda x: x['y_pos'])
        
        print(f"üìä ‡∏û‡∏ö {len(elements)} elements: {len([e for e in elements if e['type']=='text'])} text, "
              f"{len([e for e in elements if e['type']=='image'])} images, "
              f"{len([e for e in elements if e['type']=='table'])} tables")
        
        # === STEP 2.5: ‡∏£‡∏ß‡∏° text blocks ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô (‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô) ===
        # üÜï ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà text blocks ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        text_elements = [e for e in elements if e['type'] == 'text']
        if text_elements:
            # üÜï ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°: ‡∏£‡∏ß‡∏° text blocks ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            # ‡πÉ‡∏ä‡πâ threshold ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô (50 pixels) ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏° chunks ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (< 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
            merged_text_chunks = []
            current_chunk_texts = []
            current_chunk_y_pos = None
            current_chunk_bbox = None
            Y_POS_THRESHOLD = 50  # üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 20 ‡πÄ‡∏õ‡πá‡∏ô 50 pixels ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏° chunks ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            MAX_CHUNK_LENGTH = 2000  # üÜï ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á chunk (2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô chunks ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            
            for text_elem in text_elements:
                y_pos = text_elem['y_pos']
                text_content = text_elem['data']['text']
                bbox = text_elem['data'].get('bbox')
                text_length = len(text_content) if text_content else 0
                
                # üÜï ‡∏ñ‡πâ‡∏≤ chunk ‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (< 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö chunk ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏™‡∏°‡∏≠ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤ y_pos ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö block ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô
                should_merge = False
                if current_chunk_y_pos is None:
                    should_merge = True  # block ‡πÅ‡∏£‡∏Å
                elif abs(y_pos - current_chunk_y_pos) <= Y_POS_THRESHOLD:
                    should_merge = True  # y_pos ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô
                elif text_length < 100:
                    # üÜï ‡∏ñ‡πâ‡∏≤ chunk ‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö chunk ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÅ‡∏°‡πâ y_pos ‡∏à‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
                    # ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏´‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 100 pixels)
                    if abs(y_pos - current_chunk_y_pos) <= 100:
                        should_merge = True
                
                if should_merge:
                    # üÜï ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏£‡∏ß‡∏° text ‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß chunk ‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    potential_text = " ".join(current_chunk_texts + [text_content])
                    if len(potential_text) > MAX_CHUNK_LENGTH:
                        # ‡∏ñ‡πâ‡∏≤ chunk ‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å chunk ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏¥‡πà‡∏° chunk ‡πÉ‡∏´‡∏°‡πà
                        if current_chunk_texts:
                            merged_text = " ".join(current_chunk_texts)
                            merged_text_chunks.append({
                                'text': merged_text,
                                'y_pos': current_chunk_y_pos,
                                'bbox': current_chunk_bbox
                            })
                        # ‡πÄ‡∏£‡∏¥‡πà‡∏° chunk ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ text ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                        current_chunk_texts = [text_content]
                        current_chunk_y_pos = y_pos
                        current_chunk_bbox = bbox
                    else:
                        # ‡∏ñ‡πâ‡∏≤ chunk ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° text ‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                        current_chunk_texts.append(text_content)
                        if current_chunk_bbox is None:
                            current_chunk_bbox = bbox
                        current_chunk_y_pos = y_pos  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó y_pos ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á block ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                else:
                    # ‡∏ñ‡πâ‡∏≤ y_pos ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà - ‡∏™‡∏£‡πâ‡∏≤‡∏á chunk ‡πÉ‡∏´‡∏°‡πà
                    if current_chunk_texts:
                        merged_text = " ".join(current_chunk_texts)
                        merged_text_chunks.append({
                            'text': merged_text,
                            'y_pos': current_chunk_y_pos,
                            'bbox': current_chunk_bbox
                        })
                    # ‡πÄ‡∏£‡∏¥‡πà‡∏° chunk ‡πÉ‡∏´‡∏°‡πà
                    current_chunk_texts = [text_content]
                    current_chunk_y_pos = y_pos
                    current_chunk_bbox = bbox
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° chunk ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            if current_chunk_texts:
                merged_text = " ".join(current_chunk_texts)
                merged_text_chunks.append({
                    'text': merged_text,
                    'y_pos': current_chunk_y_pos,
                    'bbox': current_chunk_bbox
                })
            
            # üÜï ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ chunks ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (< 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö chunks ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô
            # ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 2: ‡∏£‡∏ß‡∏° chunks ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏±‡∏ö chunks ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô
            final_merged_chunks = []
            for i, chunk in enumerate(merged_text_chunks):
                chunk_text = chunk['text']
                chunk_length = len(chunk_text) if chunk_text else 0
                chunk_y_pos = chunk['y_pos']
                
                # ‡∏ñ‡πâ‡∏≤ chunk ‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (< 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ chunk ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô
                if chunk_length < 100 and i < len(merged_text_chunks) - 1:
                    next_chunk = merged_text_chunks[i + 1]
                    next_y_pos = next_chunk['y_pos']
                    next_text = next_chunk['text']
                    next_length = len(next_text) if next_text else 0
                    # ‡∏ñ‡πâ‡∏≤ y_pos ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô (‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 100 pixels) ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô
                    if abs(next_y_pos - chunk_y_pos) <= 100:
                        # üÜï ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß chunk ‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                        combined_text = chunk_text + " " + next_text
                        if len(combined_text) <= MAX_CHUNK_LENGTH:
                            # ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö chunk ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                            final_merged_chunks.append({
                                'text': combined_text,
                                'y_pos': chunk_y_pos,
                                'bbox': chunk.get('bbox')
                            })
                            # ‡∏Ç‡πâ‡∏≤‡∏° chunk ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß)
                            merged_text_chunks[i + 1] = None  # mark as merged
                        else:
                            # ‡∏ñ‡πâ‡∏≤ chunk ‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö chunk ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏ß‡πâ
                            final_merged_chunks.append(chunk)
                    else:
                        final_merged_chunks.append(chunk)
                else:
                    # ‡∏ñ‡πâ‡∏≤ chunk ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å mark ‡πÄ‡∏õ‡πá‡∏ô None (‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
                    if chunk is not None:
                        final_merged_chunks.append(chunk)
            
            # ‡∏Å‡∏£‡∏≠‡∏á None ‡∏≠‡∏≠‡∏Å
            final_merged_chunks = [c for c in final_merged_chunks if c is not None]
            
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà text elements ‡πÄ‡∏î‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ merged chunks
            # ‡∏•‡∏ö text elements ‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å elements list
            elements = [e for e in elements if e['type'] != 'text']
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° merged text chunks ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            for merged_chunk in final_merged_chunks:
                elements.append({
                    'type': 'text_merged',
                    'y_pos': merged_chunk['y_pos'],
                    'data': {
                        'text': merged_chunk['text'],
                        'bbox': merged_chunk.get('bbox')
                    }
                })
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å merge
            elements.sort(key=lambda x: x['y_pos'])
            print(f"üîÑ ‡∏£‡∏ß‡∏° text blocks ‡πÄ‡∏õ‡πá‡∏ô {len(final_merged_chunks)} chunks (‡∏à‡∏≤‡∏Å {len(text_elements)} blocks ‡πÄ‡∏î‡∏¥‡∏°)")
            
            # üÜï ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á chunks
            chunk_lengths = [len(chunk['text']) for chunk in final_merged_chunks if chunk.get('text')]
            if chunk_lengths:
                avg_length = sum(chunk_lengths) / len(chunk_lengths)
                max_length = max(chunk_lengths)
                min_length = min(chunk_lengths)
                chunks_over_limit = sum(1 for length in chunk_lengths if length > MAX_CHUNK_LENGTH)
                
                print(f"   üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ chunks:")
                print(f"      - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks: {len(final_merged_chunks)}")
                print(f"      - ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_length:.0f} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                print(f"      - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {max_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                print(f"      - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {min_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                if chunks_over_limit > 0:
                    print(f"      ‚ö†Ô∏è ‡∏û‡∏ö {chunks_over_limit} chunks ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô {MAX_CHUNK_LENGTH} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                else:
                    print(f"      ‚úÖ ‡∏ó‡∏∏‡∏Å chunks ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô {MAX_CHUNK_LENGTH} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
            
            for i, chunk in enumerate(final_merged_chunks[:5], 1):  # ‡πÅ‡∏™‡∏î‡∏á 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
                chunk_length = len(chunk.get('text', ''))
                size_indicator = " ‚ö†Ô∏è ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ" if chunk_length > MAX_CHUNK_LENGTH else ""
                print(f"   üìù Merged chunk {i}: {chunk_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£{size_indicator}")
        
        # === STEP 3: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô) ===
        text_chunk_counter = 0
        image_chunk_counter = 0
        table_chunk_counter = 0
        
        for element_index, element in enumerate(elements):
            element_type = element['type']
            data = element['data']
            
            print(f"\nüìå Element {element_index + 1}/{len(elements)}: {element_type.upper()} "
                  f"(y={element['y_pos']:.1f})")
            
            if element_type == 'text' or element_type == 'text_merged':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Text Block (‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö merged)
                page_results['has_content'] = True
                text_content = data['text']
                print(f"   üìù Text: {len(text_content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                
                text_chunk = {
                    "text": text_content,
                    "type": "text",
                    "chunk_id": text_chunk_counter,
                    "page": page_num + 1,
                    "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_text_{text_chunk_counter}",
                    "bbox": convert_bbox_to_mongodb_format(data['bbox'])
                }
                page_results['text_chunks'].append(text_chunk)
                text_chunk_counter += 1
            
            elif element_type == 'image':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Image
                xref = data['xref']
                img_index = data['image_index']
                
                try:
                    print(f"   üñºÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_index + 1}...")
                    base_image = pymupdf_page.parent.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    image = Image.open(io.BytesIO(image_bytes))
                    width, height = image.size
                    print(f"   üìè ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {width}x{height} pixels")
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width * height > 1500000:
                        print(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà ({width}x{height}, {width*height:,} pixels > 1,500,000)")
                        del image, image_bytes
                        continue
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width < 50 or height < 50:
                        print(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å ({width}x{height} < 50x50)")
                        del image, image_bytes
                        continue
                    
                    # OCR (‡πÉ‡∏ä‡πâ Typhoon OCR)
                    print(f"   üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥ OCR...")
                    ocr_text = perform_ocr_on_image_bytes(image_bytes)
                    
                    if ocr_text.strip():
                        page_results['has_content'] = True
                        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                        improved_text = improve_thai_ocr_text(ocr_text)
                        
                        print(f"   üñºÔ∏è Image {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (OCR: {len(ocr_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
                        
                        # Create image chunk
                        image_chunk = {
                            "text": improved_text,
                            "type": "image",
                            "chunk_id": image_chunk_counter,
                            "page": page_num + 1,
                            "image_index": img_index + 1,
                            "original_text": ocr_text.strip(),
                            "improved_text": improved_text,
                            "image_base64": base64.b64encode(image_bytes).decode("utf-8"),
                            "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_img_{img_index + 1}",
                            "bbox": convert_bbox_to_mongodb_format(data['bbox'])
                        }
                        page_results['image_chunks'].append(image_chunk)
                        image_chunk_counter += 1
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_index + 1} (OCR ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°) - ‡∏Ç‡πâ‡∏≤‡∏°")
                    
                    # ‡∏•‡πâ‡∏≤‡∏á memory
                    del image, image_bytes
                    
                except Exception as e:
                    print(f"   ‚ùó Error processing image {img_index + 1}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            elif element_type == 'table':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Table
                # üÜï ‡πÉ‡∏ä‡πâ improved_text ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡∏à‡∏≤‡∏Å camelot) ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ text ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
                table_text = data.get('original_text') or data.get('text', '')
                improved_table_text = data.get('improved_text') or data.get('text', '')
                table_index = data['table_index']
                
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
                if not data.get('improved_text'):
                    improved_table_text = improve_thai_table_text(table_text)
                
                if improved_table_text.strip():
                    page_results['has_content'] = True
                    
                    print(f"   üìä Table {table_index + 1}: {len(improved_table_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£" + 
                          (f" (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß: {len(table_text)} ‚Üí {len(improved_table_text)})" if table_text else ""))
                    
                    # Create table chunk
                    table_chunk = {
                        "text": improved_table_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        "type": "table",
                        "chunk_id": table_chunk_counter,
                        "page": page_num + 1,
                        "table_index": table_index + 1,
                        "original_text": table_text if table_text else improved_table_text,  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢
                        "improved_text": improved_table_text,  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_table_{table_index + 1}",
                        "bbox": convert_bbox_to_mongodb_format(data.get('bbox'))
                    }
                    page_results['table_chunks'].append(table_chunk)
                    table_chunk_counter += 1
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤
        if not page_results['has_content']:
            print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏°‡∏µ text, images, ‡∏´‡∏£‡∏∑‡∏≠ tables)")
        else:
            total_chunks = (len(page_results['text_chunks']) + 
                          len(page_results['image_chunks']) + 
                          len(page_results['table_chunks']))
            print(f"\n‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à: {total_chunks} chunks")
            print(f"   üìù Text: {len(page_results['text_chunks'])} chunks")
            print(f"   üñºÔ∏è Image: {len(page_results['image_chunks'])} chunks")
            print(f"   üìä Table: {len(page_results['table_chunks'])} chunks")
        
        return page_results
        
    except Exception as e:
        print(f"‚ùó Error processing page {page_num + 1}: {e}")
        import traceback
        traceback.print_exc()
        return page_results

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
def store_page_results_to_mongodb(page_results, client, is_first_page=False):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    
    Args:
        page_results: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å process_single_page()
        client: MongoDB client (‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß)
        is_first_page: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πà‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
    """
    try:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° database ‡πÅ‡∏•‡∏∞ collections
        db_original = client[ORIGINAL_DB_NAME]
        
        orig_text_col = db_original[ORIGINAL_TEXT_COLLECTION]
        orig_image_col = db_original[ORIGINAL_IMAGE_COLLECTION]
        orig_table_col = db_original[ORIGINAL_TABLE_COLLECTION]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
        if is_first_page:
            print("üóëÔ∏è ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô MongoDB...")
            orig_text_col.delete_many({})
            orig_image_col.delete_many({})
            orig_table_col.delete_many({})
            print("‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° created_at ‡πÅ‡∏•‡∏∞ embeddings ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å chunk
        now = datetime.now()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Text Chunks
        if page_results['text_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['text_chunks'])} text chunks...")
            for chunk in page_results['text_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_text_col.insert_many(page_results['text_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['text_chunks'])} text chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Image Chunks
        if page_results['image_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['image_chunks'])} image chunks...")
            for chunk in page_results['image_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å OCR)
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö image chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_image_col.insert_many(page_results['image_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['image_chunks'])} image chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data - Table Chunks
        if page_results['table_chunks']:
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(page_results['table_chunks'])} table chunks...")
            for chunk in page_results['table_chunks']:
                chunk['created_at'] = now
                # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å text
                text_content = chunk.get('text', '')
                if text_content:
                    embedding = create_text_embedding(text_content)
                    if embedding:
                        chunk['embeddings'] = embedding
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö table chunk {chunk.get('chunk_id', 'unknown')} ‡πÑ‡∏î‡πâ")
            orig_table_col.insert_many(page_results['table_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['table_chunks'])} table chunks (‡∏û‡∏£‡πâ‡∏≠‡∏° embeddings)")
        
        return True
        
    except Exception as e:
        print(f"‚ùó Error storing page results to MongoDB: {e}")
        import traceback
        traceback.print_exc()
        return False

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí loop ‡∏ï‡πà‡∏≠)
def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Pipeline: Extract ‚Üí OCR + PyThaiNLP ‚Üí Store")
    print("üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å MongoDB ‚Üí loop ‡∏ï‡πà‡∏≠")
    print()
    
    client = None
    pymupdf_doc = None
    pdfplumber_pdf = None
    
    try:
        # === INITIALIZATION ===
        print("=== INITIALIZATION ===")
        check_memory()
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏±‡πâ‡∏á PyMuPDF ‡πÅ‡∏•‡∏∞ pdfplumber
        pymupdf_doc = fitz.open(PDF_PATH)
        pdfplumber_pdf = pdfplumber.open(PDF_PATH)
        
        total_pages = len(pymupdf_doc)
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_pages} ‡∏´‡∏ô‡πâ‡∏≤")
        
        # ‡πÄ‡∏õ‡∏¥‡∏î MongoDB connection ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á pipeline)
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        total_text_chunks = 0
        total_image_chunks = 0
        total_table_chunks = 0
        
        doc_id_counter = 1  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
        # === LOOP: More Pages (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤) ===
        print("\n=== STEP 1: PAGE-BY-PAGE PROCESSING & STORING ===")
        for page_num in range(total_pages):
            print(f"\n{'='*60}")
            print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}/{total_pages}")
            print(f"{'='*60}")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Extract)
            page_results = process_single_page(
                page_num=page_num,
                pymupdf_page=pymupdf_doc[page_num],
                pdfplumber_pdf=pdfplumber_pdf,
                doc_id_counter=doc_id_counter,
                pdf_path=PDF_PATH  # üÜï ‡∏™‡πà‡∏á pdf_path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö camelot
            )
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
            is_first_page = (page_num == 0)
            print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡∏•‡∏á MongoDB...")
            
            success = store_page_results_to_mongodb(page_results, client, is_first_page=is_first_page)
            
            if success:
                # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks
                total_text_chunks += len(page_results['text_chunks'])
                total_image_chunks += len(page_results['image_chunks'])
                total_table_chunks += len(page_results['table_chunks'])
                
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            else:
                print(f"‚ö†Ô∏è ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠...")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 ‡∏´‡∏ô‡πâ‡∏≤
            if (page_num + 1) % 5 == 0:
                check_memory()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏´‡∏° (More Pages Decision)
            if page_num < total_pages - 1:
                print(f"‚û°Ô∏è ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å {total_pages - page_num - 1} ‡∏´‡∏ô‡πâ‡∏≤")
            else:
                print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ({total_pages} ‡∏´‡∏ô‡πâ‡∏≤)")
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF
        pymupdf_doc.close()
        pdfplumber_pdf.close()
        pymupdf_doc = None
        pdfplumber_pdf = None
        
        # === ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ===
        print("\n" + "="*60)
        print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        print("="*60)
        print(f"   üìù Text chunks: {total_text_chunks}")
        print(f"   üñºÔ∏è Image chunks: {total_image_chunks}")
        print(f"   üìä Table chunks: {total_table_chunks}")
        print(f"   üìä Total chunks: {total_text_chunks + total_image_chunks + total_table_chunks}")
        
        print("\n‚úÖ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB:")
        print(f"   - Database: {ORIGINAL_DB_NAME}")
        
    except Exception as e:
        print(f"‚ùó Error in main pipeline: {e}")
        import traceback
        traceback.print_exc()
        print("üîÑ Running garbage collection...")
        gc.collect()
        check_memory()
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if client:
            try:
                db_original = client[ORIGINAL_DB_NAME]
                
                orig_text_count = db_original[ORIGINAL_TEXT_COLLECTION].count_documents({})
                orig_image_count = db_original[ORIGINAL_IMAGE_COLLECTION].count_documents({})
                orig_table_count = db_original[ORIGINAL_TABLE_COLLECTION].count_documents({})
                
                print(f"\n‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß:")
                print(f"   - Original text chunks: {orig_text_count}")
                print(f"   - Original image chunks: {orig_image_count}")
                print(f"   - Original table chunks: {orig_table_count}")
            except:
                pass
        
    finally:
        # ‡∏õ‡∏¥‡∏î MongoDB connection
        if client:
            try:
                client.close()
                print("üîå ‡∏õ‡∏¥‡∏î MongoDB connection")
            except:
                pass
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î)
        if pymupdf_doc:
            try:
                pymupdf_doc.close()
            except:
                pass
        if pdfplumber_pdf:
            try:
                pdfplumber_pdf.close()
            except:
                pass

if __name__ == "__main__":
    main()
